#!/usr/bin/env bash

# TradePulse Advanced News Collection
# ==================================
# 
# Script optimisÃ© pour la version avancÃ©e avec dÃ©duplication
# et collecte multi-sources
#
# Usage:
#   ./run_advanced_daily.sh                    # Mixed sources, 3 jours, 40 articles
#   ./run_advanced_daily.sh rss 60 5          # RSS, 60 articles, 5 jours  
#   ./run_advanced_daily.sh mixed 80 2        # Mixed, 80 articles, 2 jours
#   ./run_advanced_daily.sh newsapi 50 1      # NewsAPI only, 50 articles, 1 jour

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
PYTHON_SCRIPT="$PROJECT_ROOT/scripts/collect_news.py"
DATASETS_DIR="$PROJECT_ROOT/datasets"

# ParamÃ¨tres avec valeurs optimisÃ©es
SOURCE="${1:-mixed}"        # mixed par dÃ©faut pour diversitÃ© maximale
COUNT="${2:-40}"            # Plus d'articles par dÃ©faut
DAYS="${3:-3}"              # FenÃªtre de 3 jours pour plus de contenu
DATE=$(date +%Y%m%d)
OUTPUT_FILE="$DATASETS_DIR/news_${DATE}.csv"
CACHE_FILE="$DATASETS_DIR/.article_cache.json"

# Couleurs
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m'

log_info() { echo -e "${BLUE}â„¹ï¸  $1${NC}"; }
log_success() { echo -e "${GREEN}âœ… $1${NC}"; }
log_warning() { echo -e "${YELLOW}âš ï¸  $1${NC}"; }
log_error() { echo -e "${RED}âŒ $1${NC}"; }
log_highlight() { echo -e "${PURPLE}ğŸš€ $1${NC}"; }

# Banner amÃ©liorÃ©
echo -e "${PURPLE}"
echo "ğŸ¤– TradePulse Advanced News Collector"
echo "====================================="
echo "Multi-sources â€¢ DÃ©duplication â€¢ FenÃªtre temporelle"
echo -e "${NC}"

# VÃ©rifications
log_info "VÃ©rification de l'environnement..."
mkdir -p "$DATASETS_DIR"

if [[ ! -f "$PYTHON_SCRIPT" ]]; then
    log_error "Script collect_news.py non trouvÃ©: $PYTHON_SCRIPT"
    exit 1
fi

# Installation automatique des dÃ©pendances selon la source
log_info "VÃ©rification des dÃ©pendances pour source: $SOURCE"

if [[ "$SOURCE" == "rss" || "$SOURCE" == "mixed" ]]; then
    if ! python3 -c "import feedparser" &> /dev/null; then
        log_warning "feedparser manquant - installation..."
        pip install feedparser || {
            log_error "Ã‰chec installation feedparser"
            exit 1
        }
        log_success "feedparser installÃ©"
    fi
fi

if [[ "$SOURCE" == "newsapi" || "$SOURCE" == "mixed" ]]; then
    if ! python3 -c "import requests" &> /dev/null; then
        log_warning "requests manquant - installation..."
        pip install requests || {
            log_error "Ã‰chec installation requests"
            exit 1
        }
        log_success "requests installÃ©"
    fi
    
    if [[ "$SOURCE" == "newsapi" && -z "${NEWSAPI_KEY:-}" ]]; then
        log_warning "NEWSAPI_KEY non dÃ©finie pour source NewsAPI"
        log_info "DÃ©finissez: export NEWSAPI_KEY=your_key_here"
        log_info "Ou utilisez: ./run_advanced_daily.sh mixed (RSS + NewsAPI optionnel)"
    fi
fi

# Affichage configuration avancÃ©e
log_highlight "Configuration de collecte avancÃ©e:"
echo "  ğŸ“… Date: $DATE"
echo "  ğŸ“° Source: $SOURCE"
echo "  ğŸ“Š Articles cible: $COUNT"
echo "  ğŸ“† FenÃªtre temporelle: $DAYS jours"
echo "  ğŸ“ Sortie: $OUTPUT_FILE"
echo "  ğŸ—„ï¸  Cache dÃ©duplication: $([ -f "$CACHE_FILE" ] && echo "ActivÃ© ($(jq '.articles | length' "$CACHE_FILE" 2>/dev/null || echo "?") articles)" || echo "Nouveau")"
echo

# Gestion fichier existant
if [[ -f "$OUTPUT_FILE" ]]; then
    log_warning "Fichier existant: $OUTPUT_FILE"
    
    # Afficher un aperÃ§u du fichier existant
    if command -v python3 &> /dev/null; then
        echo "  ğŸ“Š AperÃ§u actuel:"
        python3 -c "
import pandas as pd
try:
    df = pd.read_csv('$OUTPUT_FILE')
    print(f'    Articles: {len(df)}')
    dist = df['label'].value_counts()
    for label, count in dist.items():
        print(f'    {label}: {count}')
except: pass
" 2>/dev/null
    fi
    
    read -p "Remplacer? (y/N): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        log_info "OpÃ©ration annulÃ©e"
        exit 0
    fi
fi

# Construction commande avancÃ©e
log_highlight "ğŸš€ Lancement collecte avancÃ©e..."

CMD="python3 \"$PYTHON_SCRIPT\" --source \"$SOURCE\" --count $COUNT --days $DAYS --output \"$OUTPUT_FILE\""

# Options avancÃ©es
if [[ "$SOURCE" == "newsapi" || "$SOURCE" == "mixed" ]] && [[ -n "${NEWSAPI_KEY:-}" ]]; then
    CMD="$CMD --newsapi-key \"$NEWSAPI_KEY\""
fi

# ExÃ©cution avec monitoring
echo "ğŸ”„ Commande: $CMD"
echo

if eval "$CMD" 2>&1 | tee -a "$DATASETS_DIR/collect_advanced.log"; then
    log_success "Collecte terminÃ©e!"
    
    # Statistiques dÃ©taillÃ©es
    if [[ -f "$OUTPUT_FILE" ]]; then
        log_highlight "ğŸ“ˆ Analyse du dataset gÃ©nÃ©rÃ©:"
        
        # Stats basiques
        LINES=$(wc -l < "$OUTPUT_FILE" 2>/dev/null || echo "0")
        ARTICLES=$((LINES - 1))
        echo "  ğŸ“Š Articles collectÃ©s: $ARTICLES"
        
        # Distribution et mÃ©tadonnÃ©es
        if command -v python3 &> /dev/null; then
            python3 -c "
import pandas as pd
import json
from pathlib import Path

try:
    # Dataset principal
    df = pd.read_csv('$OUTPUT_FILE')
    print('  ğŸ“ˆ Distribution des labels:')
    dist = df['label'].value_counts()
    for label, count in dist.items():
        percent = (count / len(df)) * 100
        print(f'    {label}: {count} ({percent:.1f}%)')
    
    # Longueur moyenne des textes
    avg_length = df['text'].str.len().mean()
    print(f'  ğŸ“ Longueur moyenne: {avg_length:.0f} caractÃ¨res')
    
    # MÃ©tadonnÃ©es si disponibles
    metadata_file = Path('$OUTPUT_FILE').with_suffix('.json')
    if metadata_file.exists():
        with open(metadata_file, 'r') as f:
            meta = json.load(f)
        print(f'  ğŸ—„ï¸  Cache: {meta.get(\"cache_size\", 0)} articles connus')
        print(f'  â° CrÃ©Ã©: {meta.get(\"created_at\", \"Unknown\")[:16]}')
    
except Exception as e:
    print(f'  âš ï¸  Erreur analyse: {e}')
" 2>/dev/null
        fi
        
        echo
        log_highlight "ğŸ¯ VÃ©rifications qualitÃ©:"
        
        # Validation automatique si disponible
        if [[ -f "$PROJECT_ROOT/scripts/validate_dataset.py" ]]; then
            echo "  ğŸ” Validation automatique..."
            if python3 "$PROJECT_ROOT/scripts/validate_dataset.py" "$OUTPUT_FILE" --quiet 2>/dev/null; then
                echo "  âœ… Dataset valide"
            else
                echo "  âš ï¸  Validation a dÃ©tectÃ© des problÃ¨mes (voir logs complets)"
            fi
        fi
        
        echo
        log_highlight "ğŸš€ Prochaines Ã©tapes recommandÃ©es:"
        echo "  1. ğŸ“ Ã‰dition: open news_editor.html (interface web)"
        echo "  2. ğŸ” Validation: python3 scripts/validate_dataset.py $OUTPUT_FILE"
        echo "  3. ğŸ“¤ Commit: git add $OUTPUT_FILE && git commit -m 'Add diversified dataset $DATE'"
        echo "  4. ğŸš€ Push: git push (dÃ©clenche fine-tuning automatique)"
        echo "  5. ğŸ”„ Pipeline: ./scripts/auto-pipeline.sh pipeline"
        
        # Suggestions d'optimisation
        echo
        log_info "ğŸ’¡ Suggestions pour la prochaine collecte:"
        if [[ $ARTICLES -lt $((COUNT * 8 / 10)) ]]; then
            echo "  â€¢ Augmenter la fenÃªtre temporelle: --days $((DAYS + 1))"
            echo "  â€¢ Utiliser source 'mixed' pour plus de diversitÃ©"
        fi
        if [[ "$SOURCE" != "mixed" ]]; then
            echo "  â€¢ Essayer source 'mixed' pour combiner RSS + NewsAPI"
        fi
        echo "  â€¢ VÃ©rifier le cache de dÃ©duplication: $CACHE_FILE"
        
    else
        log_error "Fichier de sortie non crÃ©Ã©"
        exit 1
    fi
else
    log_error "Ã‰chec de la collecte avancÃ©e"
    exit 1
fi

# Nettoyage et maintenance
log_info "ğŸ§¹ Nettoyage maintenance..."

# Nettoyer anciens logs
find "$DATASETS_DIR" -name "*.log" -mtime +7 -delete 2>/dev/null || true

# Nettoyer anciens datasets si trop nombreux
DATASET_COUNT=$(find "$DATASETS_DIR" -name "news_*.csv" | wc -l)
if [[ $DATASET_COUNT -gt 20 ]]; then
    log_warning "Plus de 20 datasets dÃ©tectÃ©s"
    echo "  ConsidÃ©rez archiver les anciens: find datasets/ -name 'news_*.csv' -mtime +30"
fi

log_success "ğŸ‰ Collecte avancÃ©e terminÃ©e avec succÃ¨s!"

# Afficher la commande pour rÃ©pÃ©ter la mÃªme collecte
echo
log_info "ğŸ” Pour rÃ©pÃ©ter cette collecte:"
echo "  ./run_advanced_daily.sh $SOURCE $COUNT $DAYS"