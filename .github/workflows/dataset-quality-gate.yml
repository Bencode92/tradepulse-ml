name: ğŸ” Dataset Quality Gate

on:
  # Validation lors des Pull Requests touchant des datasets
  pull_request:
    paths:
      - 'datasets/**.csv'
      - 'datasets/**.json'
      - 'scripts/validate_dataset.py'
  
  # Validation manuelle
  workflow_dispatch:
    inputs:
      dataset_path:
        description: 'Dataset to validate (relative path from repo root)'
        required: true
        default: 'datasets/financial_news_20250706.csv'
        type: string

jobs:
  validate-datasets:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    outputs:
      validation-status: ${{ steps.validation.outputs.status }}
      dataset-name: ${{ steps.validation.outputs.dataset-name }}
      error-count: ${{ steps.validation.outputs.error-count }}
      warning-count: ${{ steps.validation.outputs.warning-count }}
    
    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v4
      with:
        # RÃ©cupÃ©rer l'historique pour les diffs dans les PRs
        fetch-depth: 0

    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'

    - name: ğŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas>=2.0.0 numpy>=1.24.0
        echo "âœ… Dependencies installed"

    - name: ğŸ” Validate Datasets
      id: validation
      run: |
        set -euo pipefail  # Fail fast on any error
        
        echo "ğŸ§ª Starting dataset validation..."
        
        VALIDATION_SUCCESS=true
        TOTAL_ERRORS=0
        TOTAL_WARNINGS=0
        VALIDATED_DATASETS=()
        
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          # Validation manuelle d'un dataset spÃ©cifique
          DATASET="${{ github.event.inputs.dataset_path }}"
          echo "ğŸ¯ Manual validation of: $DATASET"
          
          if [ ! -f "$DATASET" ]; then
            echo "âŒ Dataset file not found: $DATASET"
            exit 1
          fi
          
          echo "ğŸ“Š Validating: $DATASET"
          
          # Validation dÃ©taillÃ©e avec sauvegarde des erreurs
          if python scripts/validate_dataset.py "$DATASET" --save-pr-errors --output-json "validation_report.json"; then
            echo "âœ… $DATASET passed validation"
            DATASET_NAME=$(basename "$DATASET")
            echo "dataset-name=$DATASET_NAME" >> $GITHUB_OUTPUT
          else
            echo "âŒ $DATASET failed validation"
            VALIDATION_SUCCESS=false
            DATASET_NAME=$(basename "$DATASET")
            echo "dataset-name=$DATASET_NAME" >> $GITHUB_OUTPUT
          fi
          
        else
          # Validation automatique des fichiers modifiÃ©s dans la PR
          echo "ğŸ”„ PR validation mode"
          
          # Trouver les fichiers CSV/JSON modifiÃ©s
          CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }} HEAD | grep -E '^datasets/.*\.(csv|json)$' || true)
          
          if [ -z "$CHANGED_FILES" ]; then
            echo "â„¹ï¸ No dataset files modified in this PR"
            echo "status=success" >> $GITHUB_OUTPUT
            echo "dataset-name=none" >> $GITHUB_OUTPUT
            echo "error-count=0" >> $GITHUB_OUTPUT
            echo "warning-count=0" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "ğŸ“ Modified dataset files:"
          echo "$CHANGED_FILES"
          echo ""
          
          # Valider chaque fichier modifiÃ© avec rapports dÃ©taillÃ©s
          for dataset in $CHANGED_FILES; do
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            echo "ğŸ” Validating: $dataset"
            echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            
            if [ ! -f "$dataset" ]; then
              echo "âŒ File was deleted or moved: $dataset"
              continue
            fi
            
            # Validation avec rapport JSON pour agrÃ©gation
            DATASET_BASENAME=$(basename "$dataset" .csv)
            REPORT_FILE="validation_${DATASET_BASENAME}.json"
            
            if python scripts/validate_dataset.py "$dataset" --save-pr-errors --output-json "$REPORT_FILE"; then
              echo "âœ… $dataset passed validation"
              VALIDATED_DATASETS+=("$dataset")
            else
              echo "âŒ $dataset failed validation"
              VALIDATION_SUCCESS=false
              
              # AgrÃ©ger les erreurs
              if [ -f "$REPORT_FILE" ]; then
                ERRORS=$(jq -r '.error_count // 0' "$REPORT_FILE")
                WARNINGS=$(jq -r '.warning_count // 0' "$REPORT_FILE")
                TOTAL_ERRORS=$((TOTAL_ERRORS + ERRORS))
                TOTAL_WARNINGS=$((TOTAL_WARNINGS + WARNINGS))
              fi
            fi
            echo ""
          done
          
          # Outputs pour les jobs suivants
          if [ "$VALIDATION_SUCCESS" = "true" ]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "âœ… All modified datasets passed validation!"
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "âŒ One or more datasets failed validation"
            echo "ğŸ’¡ Please fix the issues above before merging"
          fi
          
          # Derniers dataset validÃ© pour chaÃ®ner avec le fine-tuning
          if [ ${#VALIDATED_DATASETS[@]} -gt 0 ]; then
            LAST_DATASET=$(basename "${VALIDATED_DATASETS[-1]}")
            echo "dataset-name=$LAST_DATASET" >> $GITHUB_OUTPUT
          else
            echo "dataset-name=none" >> $GITHUB_OUTPUT
          fi
        fi
        
        echo "error-count=$TOTAL_ERRORS" >> $GITHUB_OUTPUT
        echo "warning-count=$TOTAL_WARNINGS" >> $GITHUB_OUTPUT
        
        # Fail le job si validation Ã©choue
        if [ "$VALIDATION_SUCCESS" != "true" ]; then
          exit 1
        fi

    - name: ğŸ“Š Generate Validation Summary
      if: always()
      run: |
        echo "ğŸ“‹ Dataset Validation Summary" | tee validation_summary.md
        echo "=============================" | tee -a validation_summary.md
        echo "" | tee -a validation_summary.md
        echo "ğŸ• **Timestamp:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" | tee -a validation_summary.md
        echo "ğŸŒ¿ **Branch:** ${{ github.head_ref || github.ref_name }}" | tee -a validation_summary.md
        echo "ğŸ‘¤ **Author:** ${{ github.actor }}" | tee -a validation_summary.md
        
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          echo "ğŸ”— **PR:** #${{ github.event.number }}" | tee -a validation_summary.md
          echo "ğŸ“ **Title:** ${{ github.event.pull_request.title }}" | tee -a validation_summary.md
        fi
        
        echo "" | tee -a validation_summary.md
        echo "ğŸ“Š **RÃ©sultats:**" | tee -a validation_summary.md
        echo "- Erreurs: ${{ steps.validation.outputs.error-count || 0 }}" | tee -a validation_summary.md  
        echo "- Avertissements: ${{ steps.validation.outputs.warning-count || 0 }}" | tee -a validation_summary.md
        echo "- Statut: ${{ steps.validation.outputs.validation-status || 'unknown' }}" | tee -a validation_summary.md
        
        echo "" | tee -a validation_summary.md
        echo "ğŸ“ **Datasets du repository:**" | tee -a validation_summary.md
        find datasets/ -name "*.csv" -o -name "*.json" | head -10 | while read file; do
          size=$(du -h "$file" 2>/dev/null | cut -f1 || echo "?")
          echo "- ğŸ“„ \`$file\` ($size)" | tee -a validation_summary.md
        done
        
        total_datasets=$(find datasets/ -name "*.csv" -o -name "*.json" | wc -l)
        echo "" | tee -a validation_summary.md
        echo "ğŸ“ˆ **Total datasets:** $total_datasets" | tee -a validation_summary.md

    - name: ğŸ’¬ Comment PR with Detailed Results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Lire le statut de validation
          const status = '${{ steps.validation.outputs.validation-status }}' || 'unknown';
          const errorCount = parseInt('${{ steps.validation.outputs.error-count }}' || '0');
          const warningCount = parseInt('${{ steps.validation.outputs.warning-count }}' || '0');
          const runUrl = `${context.payload.repository.html_url}/actions/runs/${context.runId}`;
          
          let emoji, statusText, message;
          
          if (status === 'success') {
            emoji = 'âœ…';
            statusText = 'SUCCÃˆS';
            message = 'ğŸ‰ Tous les datasets ont passÃ© la validation ! Vos donnÃ©es sont prÃªtes pour l\'entraÃ®nement.';
          } else if (status === 'failure') {
            emoji = 'âŒ';
            statusText = 'Ã‰CHEC';
            message = 'ğŸš¨ La validation des datasets a Ã©chouÃ©. Veuillez corriger les problÃ¨mes ci-dessous.';
          } else {
            emoji = 'âš ï¸';
            statusText = 'ANNULÃ‰/IGNORÃ‰';
            message = 'ğŸ¤” La validation a Ã©tÃ© annulÃ©e ou ignorÃ©e.';
          }
          
          // Lire les erreurs dÃ©taillÃ©es si disponibles
          let detailedErrors = '';
          try {
            if (fs.existsSync('validation_errors.txt')) {
              detailedErrors = fs.readFileSync('validation_errors.txt', 'utf8');
            }
          } catch (error) {
            console.log('No detailed errors file found');
          }
          
          // Lire le rÃ©sumÃ© si disponible
          let summary = '';
          try {
            if (fs.existsSync('validation_summary.md')) {
              summary = fs.readFileSync('validation_summary.md', 'utf8');
            }
          } catch (error) {
            console.log('No summary file found');
          }
          
          // Construire le commentaire
          let commentBody = `## ${emoji} Dataset Quality Gate - ${statusText}
          
          ${message}
          
          ### ğŸ“Š RÃ©sumÃ© de validation
          - **Erreurs critiques:** ${errorCount}
          - **Avertissements:** ${warningCount}
          - **Statut:** ${statusText}
          `;
          
          if (detailedErrors) {
            commentBody += `
          ### ğŸ” DÃ©tails des problÃ¨mes
          \`\`\`
          ${detailedErrors}
          \`\`\`
          `;
          }
          
          commentBody += `
          ### ğŸ”— Liens utiles
          - ğŸ“Š [Rapport complet de validation](${runUrl})
          - ğŸ“– [Guide du workflow](https://github.com/${context.repo.owner}/${context.repo.repo}/blob/main/DATASET_WORKFLOW.md)
          - ğŸ”§ Script de validation: \`scripts/validate_dataset.py\`
          
          ### ğŸ“‹ Prochaines Ã©tapes
          `;
          
          if (status === 'success') {
            commentBody += `
          âœ… **PrÃªt Ã  merger !** Le fine-tuning se lancera automatiquement aprÃ¨s le merge.
          
          ğŸš€ **Actions disponibles:**
          - Merger cette PR pour dÃ©clencher l'entraÃ®nement automatique
          - Ou lancer manuellement via Actions â†’ "TradePulse FinBERT Fine-tuning"
          `;
          } else {
            commentBody += `
          ğŸ”§ **Corrections requises:**
          1. Corrigez les erreurs listÃ©es ci-dessus
          2. Committez et pushez les modifications
          3. La validation se relancera automatiquement
          
          ğŸ’¡ **Aide:**
          - Testez localement: \`python scripts/validate_dataset.py votre_dataset.csv\`
          - Consultez le [guide du workflow](https://github.com/${context.repo.owner}/${context.repo.repo}/blob/main/DATASET_WORKFLOW.md)
          `;
          }
          
          commentBody += `
          
          ---
          *ğŸ¤– Commentaire automatique gÃ©nÃ©rÃ© par Dataset Quality Gate*
          `;
          
          // Poster le commentaire
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: commentBody
          });

    - name: ğŸ“ˆ Create GitHub Check Run
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const status = '${{ steps.validation.outputs.validation-status }}' || 'unknown';
          const errorCount = parseInt('${{ steps.validation.outputs.error-count }}' || '0');
          const warningCount = parseInt('${{ steps.validation.outputs.warning-count }}' || '0');
          
          let conclusion, title, summary;
          
          if (status === 'success') {
            conclusion = 'success';
            title = 'âœ… Dataset validation passed';
            summary = `All datasets are valid and ready for training!\n\n**Results:**\n- Errors: ${errorCount}\n- Warnings: ${warningCount}`;
          } else {
            conclusion = 'failure';
            title = 'âŒ Dataset validation failed';  
            summary = `Dataset validation failed. Please fix the issues before merging.\n\n**Results:**\n- Errors: ${errorCount}\n- Warnings: ${warningCount}`;
          }
          
          await github.rest.checks.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            name: 'Dataset Quality Gate',
            head_sha: context.payload.pull_request.head.sha,
            status: 'completed',
            conclusion: conclusion,
            output: {
              title: title,
              summary: summary
            }
          });

    # PrÃ©parer les artifacts pour debug et traÃ§abilitÃ©
    - name: ğŸ“¤ Upload Validation Artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: dataset-validation-${{ github.event.number || 'manual' }}-${{ github.run_id }}
        path: |
          validation_*.json
          validation_errors.txt
          validation_summary.md
          datasets/
        retention-days: 30

  # Job pour prÃ©-approuver automatiquement si tout est OK (optionnel)
  auto-approve:
    runs-on: ubuntu-latest
    needs: validate-datasets
    if: github.event_name == 'pull_request' && needs.validate-datasets.outputs.validation-status == 'success' && github.actor != github.repository_owner
    
    steps:
    - name: ğŸ¯ Auto-approve high-quality datasets
      uses: actions/github-script@v7
      with:
        script: |
          const errorCount = parseInt('${{ needs.validate-datasets.outputs.error-count }}');
          const warningCount = parseInt('${{ needs.validate-datasets.outputs.warning-count }}');
          
          // Auto-approve seulement si 0 erreurs et < 3 warnings
          if (errorCount === 0 && warningCount <= 2) {
            try {
              await github.rest.pulls.createReview({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: context.issue.number,
                event: 'APPROVE',
                body: 'ğŸ¤– **Auto-approval**: Dataset validation passed with excellent quality!\n\nâœ… 0 errors, minimal warnings\nğŸš€ Ready for automatic merge and training'
              });
              
              // Ajouter un label pour faciliter l'identification
              await github.rest.issues.addLabels({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                labels: ['auto-approved', 'high-quality-dataset']
              });
              
            } catch (error) {
              console.log('Auto-approval failed (probably missing permissions):', error.message);
            }
          }
