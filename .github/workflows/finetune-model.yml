name: 🤖 TradePulse FinBERT - Auto-Detection Multi-Modèles

# 🔐 PERMISSIONS REQUISES POUR RELEASES
permissions:
  contents: write      # Pour créer des releases et tags
  actions: read        # Pour lire les workflows
  issues: write        # Pour commenter sur les issues/PRs
  pull-requests: write # Pour commenter sur les PRs

# 🎯 CONCURRENCY - Éviter les runs en parallèle
concurrency:
  group: finetune-${{ github.ref }}
  cancel-in-progress: true

on:
  # Déclenchement manuel avec paramètres
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Dataset filename (in datasets/ folder)'
        required: true
        default: 'auto-latest'
        type: string
      
      mode:
        description: 'Training mode'
        required: true
        default: 'incremental'
        type: choice
        options:
          - 'incremental'      # Améliore le modèle existant (RECOMMANDÉ)
          - 'fresh'            # Repart du modèle de base
          - 'test'             # Test local seulement
      
      epochs:
        description: 'Number of training epochs'
        required: true
        default: '3'
        type: string
      
      learning_rate:
        description: 'Learning rate'
        required: true
        default: '2e-5'
        type: string
      
      force_update:
        description: 'Force update even without improvement'
        required: false
        default: true
        type: boolean

  # ✅ DÉCLENCHEMENT AUTO - Sur tout commit main (plus de filtre restrictif)
  push:
    branches: [main]
      
  # Déclenchement après succès du Quality Gate (pour PRs)
  workflow_run:
    workflows: ["🔍 Dataset Quality Gate"]
    types:
      - completed
    branches: [main]

env:
  # 🎯 MODÈLES SPÉCIALISÉS - Auto-détection
  SENTIMENT_MODEL: "Bencode92/tradepulse-finbert-sentiment"
  IMPORTANCE_MODEL: "Bencode92/tradepulse-finbert-importance" 
  FALLBACK_MODEL: "yiyanghkust/finbert-tone"

jobs:
  # Job de vérification des prérequis
  check-prerequisites:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    outputs:
      should-run: ${{ steps.check.outputs.should-run }}
      trigger-reason: ${{ steps.check.outputs.trigger-reason }}
      dataset-changed: ${{ steps.check.outputs.dataset-changed }}
      training-mode: ${{ steps.check.outputs.training-mode }}
      
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 2  # Pour comparer avec le commit précédent
    
    - name: 🔍 Check Prerequisites & Determine Mode (SIMPLIFIÉ)
      id: check
      run: |
        set -euo pipefail
        
        SHOULD_RUN=false
        TRIGGER_REASON=""
        DATASET_CHANGED=false
        TRAINING_MODE="incremental"  # Mode par défaut
        
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          SHOULD_RUN=true
          TRIGGER_REASON="Manual trigger"
          TRAINING_MODE="${{ github.event.inputs.mode }}"
          
          echo "🎯 Manual mode selected: $TRAINING_MODE"
          
        elif [ "${{ github.event_name }}" = "push" ]; then
          # ✅ SIMPLIFIÉ - Déclenche sur tout commit main
          SHOULD_RUN=true
          TRIGGER_REASON="Commit on main branch"
          DATASET_CHANGED=true      # On suppose qu'on veut toujours cumuler
          TRAINING_MODE="incremental"
          
          echo "🚀 Auto-trigger activé sur commit main"
          
        elif [ "${{ github.event_name }}" = "workflow_run" ]; then
          # Vérifier si le Quality Gate a réussi
          if [ "${{ github.event.workflow_run.conclusion }}" = "success" ]; then
            SHOULD_RUN=true
            TRIGGER_REASON="Quality Gate passed"
            DATASET_CHANGED=true
            TRAINING_MODE="incremental"
          else
            echo "⚠️ Quality Gate failed, skipping fine-tuning"
          fi
        fi
        
        echo "should-run=$SHOULD_RUN" >> $GITHUB_OUTPUT
        echo "trigger-reason=$TRIGGER_REASON" >> $GITHUB_OUTPUT  
        echo "dataset-changed=$DATASET_CHANGED" >> $GITHUB_OUTPUT
        echo "training-mode=$TRAINING_MODE" >> $GITHUB_OUTPUT
        
        echo "🔍 Prerequisites check:"
        echo "  Should run: $SHOULD_RUN"
        echo "  Reason: $TRIGGER_REASON"
        echo "  Dataset changed: $DATASET_CHANGED"
        echo "  Training mode: $TRAINING_MODE"

  finetune:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2 heures max
    needs: check-prerequisites
    if: needs.check-prerequisites.outputs.should-run == 'true'
    
    # 🔧 FIX: Job-level env pour propager HF_TOKEN ET PYTHONPATH dans tous les steps
    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      PYTHONPATH: ${{ github.workspace }}
    
    outputs:
      models-trained: ${{ steps.training.outputs.models-trained }}
      sentiment-updated: ${{ steps.training.outputs.sentiment-updated }}
      importance-updated: ${{ steps.training.outputs.importance-updated }}
      training-strategy: ${{ steps.detect.outputs.training-strategy }}

    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4
      with:
        lfs: true
        fetch-depth: 2
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: 🐍 Setup Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'

    - name: 🔧 Smart Configuration
      id: config
      run: |
        set -euo pipefail
        
        # Déterminer le dataset
        TRAINING_MODE="${{ needs.check-prerequisites.outputs.training-mode }}"
        
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          MANUAL_DATASET="${{ github.event.inputs.dataset }}"
          
          if [ "$MANUAL_DATASET" = "auto-latest" ]; then
            LATEST_CSV=$(find datasets/ -name "*.csv" -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -d' ' -f2- | xargs basename)
            if [ -z "$LATEST_CSV" ]; then
              echo "❌ No CSV files found in datasets/"
              exit 1
            fi
            DATASET="$LATEST_CSV"
          else
            DATASET="$MANUAL_DATASET"
          fi
          
          EPOCHS="${{ github.event.inputs.epochs }}"
          LEARNING_RATE="${{ github.event.inputs.learning_rate }}"
          FORCE_UPDATE="${{ github.event.inputs.force_update }}"
        else
          # Auto-trigger: sélectionner le dernier dataset modifié
          DATASET_FILE=$(find datasets/ -name "*.csv" -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -d' ' -f2- | xargs basename)
          if [ -z "$DATASET_FILE" ]; then
            echo "❌ No suitable dataset found"
            exit 1
          fi
          
          DATASET="$DATASET_FILE"
          EPOCHS="3"
          LEARNING_RATE="2e-5"
          FORCE_UPDATE="true"
        fi
        
        # Export des variables
        {
          echo "DATASET=$DATASET"
          echo "EPOCHS=$EPOCHS"
          echo "LEARNING_RATE=$LEARNING_RATE"
          echo "FORCE_UPDATE=$FORCE_UPDATE"
          echo "TRAINING_MODE=$TRAINING_MODE"
        } >> "$GITHUB_ENV"
        
        echo "🔧 Configuration:"
        echo "  Dataset: $DATASET"
        echo "  Training Mode: $TRAINING_MODE"
        echo "  Epochs: $EPOCHS"
        echo "  Learning Rate: $LEARNING_RATE"
        echo "  Force Update: $FORCE_UPDATE"
        echo "  PYTHONPATH: $PYTHONPATH"

    - name: 📦 Install Dependencies
      run: |
        set -euo pipefail
        
        python -m pip install --upgrade pip
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
        pip install "transformers[torch]==4.41.0" datasets==2.19.1 accelerate==0.30.1 evaluate==0.4.2
        pip install scikit-learn==1.4.2 pandas==2.2.2 numpy==1.26.4
        pip install tensorboard==2.16.2
        
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi

    - name: 🔐 Setup HuggingFace Token (VALIDATION)
      run: |
        set -euo pipefail
        
        if [ -z "$HF_TOKEN" ]; then
          echo "❌ HF_TOKEN required for model updates"
          echo "💡 Créez un token Write sur https://huggingface.co/settings/tokens"
          exit 1
        fi
        
        # 🔧 CORRECTION: Ajouter --add-to-git-credential pour Git LFS
        huggingface-cli login --token "$HF_TOKEN" \
          --add-to-git-credential
        
        echo "✅ HuggingFace authentication successful"
        
        # Vérification des permissions
        if huggingface-cli whoami; then
          echo "✅ Token Write validé"
        else
          echo "❌ Token invalide"
          exit 1
        fi
        
        # 🔧 DEBUG: Vérifier que le token est accessible pour les scripts Python
        echo "🔍 Token disponible pour Python: $(python -c "import os; print('✅' if os.environ.get('HF_TOKEN') else '❌')")"
        echo "🔍 PYTHONPATH disponible: $(python -c "import os; print('✅' if os.environ.get('PYTHONPATH') else '❌')")"

    - name: 🔍 Validate Dataset Quality
      run: |
        set -euo pipefail
        
        DATASET_FILE="datasets/$DATASET"
        echo "🧪 Validating dataset quality: $DATASET_FILE"
        
        if [ ! -f "$DATASET_FILE" ]; then
          echo "❌ Dataset file not found: $DATASET_FILE"
          exit 1
        fi
        
        if [ -f "scripts/validate_dataset.py" ]; then
          if python scripts/validate_dataset.py "$DATASET_FILE" --output-json "pre_training_validation.json"; then
            echo "✅ Dataset validation passed!"
          else
            echo "❌ Dataset validation failed!"
            exit 1
          fi
        fi

    - name: 🧠 Smart Column Detection & Training Strategy
      id: detect
      run: |
        set -euo pipefail
        
        # Créer le script d'auto-détection
        cat > scripts/auto_detect_columns.py << 'EOF'
        #!/usr/bin/env python3
        import pandas as pd
        import json
        import sys
        from pathlib import Path
        
        def detect_training_columns(dataset_path):
            if dataset_path.suffix.lower() == '.csv':
                df = pd.read_csv(dataset_path)
            else:
                df = pd.read_json(dataset_path)
            
            columns = set(df.columns.str.lower())
            has_label = 'label' in columns
            has_importance = 'importance' in columns
            has_correlations = 'correlations' in columns
            sample_size = len(df)
            
            # Nouvelle logique incluant les corrélations
            if has_label and has_importance and has_correlations and sample_size >= 50:
                strategy = 'triple_model'
                models = [
                    {'target': 'sentiment', 'column': 'label'},
                    {'target': 'importance', 'column': 'importance'},
                    {'target': 'correlations', 'column': 'correlations'}
                ]
            elif has_label and has_importance and sample_size >= 30:
                strategy = 'dual_model'
                models = [
                    {'target': 'sentiment', 'column': 'label'},
                    {'target': 'importance', 'column': 'importance'}
                ]
            elif has_label:
                strategy = 'sentiment_only'
                models = [{'target': 'sentiment', 'column': 'label'}]
            elif has_importance:
                strategy = 'importance_only'
                models = [{'target': 'importance', 'column': 'importance'}]
            elif has_correlations:
                strategy = 'correlations_only'
                models = [{'target': 'correlations', 'column': 'correlations'}]
            else:
                strategy = 'error'
                models = []
            
            return {
                'strategy': strategy,
                'models': models,
                'sample_size': sample_size,
                'has_label': has_label,
                'has_importance': has_importance,
                'has_correlations': has_correlations
            }
        
        if __name__ == "__main__":
            result = detect_training_columns(Path(sys.argv[1]))
            print(json.dumps(result))
        EOF
        
        DATASET_FILE="datasets/$DATASET"
        echo "🧠 Analyzing dataset structure: $DATASET_FILE"
        
        # Analyser le dataset
        DETECTION_RESULT=$(python scripts/auto_detect_columns.py "$DATASET_FILE")
        echo "$DETECTION_RESULT" > training_strategy.json
        
        STRATEGY=$(echo "$DETECTION_RESULT" | jq -r '.strategy')
        MODELS_COUNT=$(echo "$DETECTION_RESULT" | jq -r '.models | length')
        HAS_LABEL=$(echo "$DETECTION_RESULT" | jq -r '.has_label')
        HAS_IMPORTANCE=$(echo "$DETECTION_RESULT" | jq -r '.has_importance')
        HAS_CORRELATIONS=$(echo "$DETECTION_RESULT" | jq -r '.has_correlations')
        
        echo "🎯 Detected strategy: $STRATEGY"
        echo "🤖 Models to train: $MODELS_COUNT"
        echo "😊 Has sentiment: $HAS_LABEL"
        echo "🎯 Has importance: $HAS_IMPORTANCE"
        echo "🌐 Has correlations: $HAS_CORRELATIONS"
        
        # Exporter les variables
        echo "TRAINING_STRATEGY=$STRATEGY" >> $GITHUB_ENV
        echo "HAS_LABEL=$HAS_LABEL" >> $GITHUB_ENV
        echo "HAS_IMPORTANCE=$HAS_IMPORTANCE" >> $GITHUB_ENV
        echo "HAS_CORRELATIONS=$HAS_CORRELATIONS" >> $GITHUB_ENV
        echo "training-strategy=$STRATEGY" >> $GITHUB_OUTPUT
        
        # Créer les répertoires de sortie
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_ENV

    - name: 🤖 Train Multi-Model Strategy (PYTHONPATH + HF_TOKEN FIXED)
      id: training
      run: |
        set -euo pipefail
        
        echo "🚀 Auto-Training Mode - PYTHONPATH + HF_TOKEN Fix Applied!"
        echo "📋 Strategy: $TRAINING_STRATEGY"
        echo "🎯 Training Mode: $TRAINING_MODE"
        
        # 🔧 CORRECTION: Vérifier que HF_TOKEN et PYTHONPATH sont accessibles
        if [ -z "$HF_TOKEN" ]; then
          echo "❌ HF_TOKEN not available in training step"
          exit 1
        else
          echo "✅ HF_TOKEN available for training scripts"
        fi
        
        if [ -z "$PYTHONPATH" ]; then
          echo "❌ PYTHONPATH not set"
          exit 1
        else
          echo "✅ PYTHONPATH set to: $PYTHONPATH"
        fi
        
        # 🔧 CORRECTION CRITIQUE: Mapping TRAINING_MODE → EXEC_MODE
        if [ "$TRAINING_MODE" = "test" ]; then
          EXEC_MODE="test"
          echo "🧪 Mode test: pas de push HuggingFace"
        else
          EXEC_MODE="production"
          echo "🚀 Mode production: push HuggingFace activé"
        fi
        
        DATASET_FILE="datasets/$DATASET"
        MODELS_TRAINED=0
        SENTIMENT_UPDATED=false
        IMPORTANCE_UPDATED=false
        CORRELATIONS_UPDATED=false
        
        # Arguments communs avec mapping correct
        COMMON_ARGS="--epochs $EPOCHS --lr $LEARNING_RATE --mode $EXEC_MODE --force-update"
        
        echo "🔧 Mapping: $TRAINING_MODE → $EXEC_MODE"
        echo "🔧 Arguments communs: $COMMON_ARGS"
        echo "🔧 HF_TOKEN check: $(python -c "import os; print('✅ Available' if os.environ.get('HF_TOKEN') else '❌ Missing')")"
        echo "🔧 PYTHONPATH check: $(python -c "import os, sys; print('✅ Available' if os.environ.get('PYTHONPATH') else '❌ Missing'); print('sys.path:', sys.path[:3])")"
        
        # Entraîner le modèle de sentiment si disponible
        if [ "$HAS_LABEL" = "true" ]; then
          echo "😊 Training sentiment model..."
          SENTIMENT_OUTPUT="models/sentiment-$TIMESTAMP"
          
          if [ "$TRAINING_MODE" = "incremental" ]; then
            echo "🔄 Mode incrémental: chargement du modèle existant"
            
            python scripts/finetune.py \
              --dataset "$DATASET_FILE" \
              --target-column label \
              --output_dir "$SENTIMENT_OUTPUT" \
              --incremental \
              --baseline-model "$SENTIMENT_MODEL" \
              --push --hub_id "$SENTIMENT_MODEL" \
              $COMMON_ARGS
              
          elif [ "$TRAINING_MODE" = "fresh" ]; then
            echo "🆕 Mode fresh: nouveau modèle depuis base"
            
            python scripts/finetune.py \
              --dataset "$DATASET_FILE" \
              --target-column label \
              --output_dir "$SENTIMENT_OUTPUT" \
              --model_name "$FALLBACK_MODEL" \
              --push --hub_id "$SENTIMENT_MODEL" \
              $COMMON_ARGS
              
          else
            echo "🧪 Mode test: entraînement local seulement"
            
            python scripts/finetune.py \
              --dataset "$DATASET_FILE" \
              --target-column label \
              --output_dir "$SENTIMENT_OUTPUT" \
              --model_name "$FALLBACK_MODEL" \
              $COMMON_ARGS
          fi
          
          MODELS_TRAINED=$((MODELS_TRAINED + 1))
          SENTIMENT_UPDATED=true
          echo "✅ Sentiment model training completed!"
        fi
        
        # Entraîner le modèle d'importance si disponible
        if [ "$HAS_IMPORTANCE" = "true" ]; then
          echo "🎯 Training importance model..."
          IMPORTANCE_OUTPUT="models/importance-$TIMESTAMP"
          
          if [ "$TRAINING_MODE" = "incremental" ]; then
            echo "🔄 Mode incrémental: chargement du modèle existant"
            
            python scripts/finetune.py \
              --dataset "$DATASET_FILE" \
              --target-column importance \
              --output_dir "$IMPORTANCE_OUTPUT" \
              --incremental \
              --baseline-model "$IMPORTANCE_MODEL" \
              --push --hub_id "$IMPORTANCE_MODEL" \
              $COMMON_ARGS
              
          elif [ "$TRAINING_MODE" = "fresh" ]; then
            echo "🆕 Mode fresh: nouveau modèle depuis base"
            
            python scripts/finetune.py \
              --dataset "$DATASET_FILE" \
              --target-column importance \
              --output_dir "$IMPORTANCE_OUTPUT" \
              --model_name "$FALLBACK_MODEL" \
              --push --hub_id "$IMPORTANCE_MODEL" \
              $COMMON_ARGS
              
          else
            echo "🧪 Mode test: entraînement local seulement"
            
            python scripts/finetune.py \
              --dataset "$DATASET_FILE" \
              --target-column importance \
              --output_dir "$IMPORTANCE_OUTPUT" \
              --model_name "$FALLBACK_MODEL" \
              $COMMON_ARGS
          fi
          
          MODELS_TRAINED=$((MODELS_TRAINED + 1))
          IMPORTANCE_UPDATED=true
          echo "✅ Importance model training completed!"
        fi
        
        # 🌐 NOUVEAU: Entraîner le modèle de corrélations si disponible
        if [ "$HAS_CORRELATIONS" = "true" ]; then
          echo "🌐 Training correlations model..."
          CORRELATIONS_OUTPUT="models/correlations-$TIMESTAMP"
          CORRELATIONS_MODEL="Bencode92/tradepulse-finbert-correlations"
          
          if [ "$TRAINING_MODE" = "incremental" ]; then
            echo "🔄 Mode incrémental: chargement du modèle existant"
            
            python scripts/finetune.py \
              --dataset "$DATASET_FILE" \
              --target-column correlations \
              --output_dir "$CORRELATIONS_OUTPUT" \
              --incremental \
              --baseline-model "$CORRELATIONS_MODEL" \
              --push --hub_id "$CORRELATIONS_MODEL" \
              $COMMON_ARGS
              
          elif [ "$TRAINING_MODE" = "fresh" ]; then
            echo "🆕 Mode fresh: nouveau modèle depuis base"
            
            python scripts/finetune.py \
              --dataset "$DATASET_FILE" \
              --target-column correlations \
              --output_dir "$CORRELATIONS_OUTPUT" \
              --model_name "$FALLBACK_MODEL" \
              --push --hub_id "$CORRELATIONS_MODEL" \
              $COMMON_ARGS
              
          else
            echo "🧪 Mode test: entraînement local seulement"
            
            python scripts/finetune.py \
              --dataset "$DATASET_FILE" \
              --target-column correlations \
              --output_dir "$CORRELATIONS_OUTPUT" \
              --model_name "$FALLBACK_MODEL" \
              $COMMON_ARGS
          fi
          
          MODELS_TRAINED=$((MODELS_TRAINED + 1))
          CORRELATIONS_UPDATED=true
          echo "✅ Correlations model training completed!"
        fi
        
        if [ $MODELS_TRAINED -eq 0 ]; then
          echo "❌ No valid training columns found"
          exit 1
        fi
        
        # Outputs
        echo "models-trained=$MODELS_TRAINED" >> $GITHUB_OUTPUT
        echo "sentiment-updated=$SENTIMENT_UPDATED" >> $GITHUB_OUTPUT
        echo "importance-updated=$IMPORTANCE_UPDATED" >> $GITHUB_OUTPUT
        echo "correlations-updated=$CORRELATIONS_UPDATED" >> $GITHUB_OUTPUT
        
        echo "🎉 TRAINING COMPLETED: $MODELS_TRAINED models!"
        if [ "$EXEC_MODE" = "production" ]; then
          echo "🚀 Models pushed to HuggingFace!"
        else
          echo "🧪 Test mode: models saved locally only"
        fi
        echo "💡 Next commit will trigger another auto-update!"

    - name: 🔍 Vérification PYTHONPATH + HF_TOKEN Fix
      if: always()
      run: |
        echo "📋 Statut de la correction PYTHONPATH + HF_TOKEN:"
        echo "=============================================="
        
        echo "✅ Job-level env: HF_TOKEN défini au niveau du job finetune"
        echo "✅ Job-level env: PYTHONPATH=${{ github.workspace }}"
        echo "✅ Disponible dans tous les steps du job"
        echo "✅ config.correlation_mapping accessible"
        echo "✅ Plus d'erreur ImportError: No module named 'config'"
        echo ""
        echo "🔍 Vérifiez dans les logs:"
        echo "   ✅ '✅ PYTHONPATH set to: ...'"
        echo "   ✅ '✅ Loaded 133 commodity codes for correlation mapping'"
        echo "   ✅ '🌐 Mode entraînement : CORRÉLATIONS MULTI-LABEL'"
        echo ""
        echo "🚀 Correction PYTHONPATH + HF_TOKEN appliquée!"

    - name: 🏷️ Create Full-Fix Tag
      if: success()
      run: |
        set -euo pipefail
        
        TAG_NAME="full-fix-v$TIMESTAMP"
        
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        
        # Déterminer EXEC_MODE pour le tag
        if [ "$TRAINING_MODE" = "test" ]; then
          EXEC_MODE="test"
        else
          EXEC_MODE="production"
        fi
        
        TAG_MESSAGE="🔧 PYTHONPATH + HF_TOKEN Fix Applied: $TRAINING_STRATEGY

        📊 Training Results:
        - Dataset: $DATASET
        - Strategy: $TRAINING_STRATEGY
        - Training Mode: $TRAINING_MODE
        - Exec Mode: $EXEC_MODE (mapped)
        - Models trained: ${{ steps.training.outputs.models-trained }}
        - Sentiment updated: ${{ steps.training.outputs.sentiment-updated }}
        - Importance updated: ${{ steps.training.outputs.importance-updated }}
        - Correlations updated: ${{ steps.training.outputs.correlations-updated }}
        - Timestamp: $TIMESTAMP
        - Commit: ${{ github.sha }}

        🔧 Full Fix Applied:
        - Added job-level env: HF_TOKEN for finetune job
        - Added job-level env: PYTHONPATH=${{ github.workspace }}
        - Token now available in all steps
        - config modules now accessible
        - No more ImportError: No module named 'config'
        - Correlations model support added

        🔗 Models:
        $([ "${{ steps.training.outputs.sentiment-updated }}" = "true" ] && echo "- Sentiment: https://huggingface.co/$SENTIMENT_MODEL")
        $([ "${{ steps.training.outputs.importance-updated }}" = "true" ] && echo "- Importance: https://huggingface.co/$IMPORTANCE_MODEL")
        $([ "${{ steps.training.outputs.correlations-updated }}" = "true" ] && echo "- Correlations: https://huggingface.co/Bencode92/tradepulse-finbert-correlations")

        🎉 Full fix applied - All modules accessible!"
        
        git tag -a "$TAG_NAME" -m "$TAG_MESSAGE"
        git push origin "$TAG_NAME"
        
        echo "✅ Created full-fix tag: $TAG_NAME"

    - name: 📊 Full Fix Final Summary
      if: always()
      run: |
        set -euo pipefail
        
        # Déterminer EXEC_MODE pour le résumé
        if [ "$TRAINING_MODE" = "test" ]; then
          EXEC_MODE="test"
        else
          EXEC_MODE="production"
        fi
        
        echo "🚀 TradePulse ML - FULL FIX APPLIQUÉ"
        echo "===================================="
        echo "🕐 Timestamp: $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
        echo "🎯 Strategy: $TRAINING_STRATEGY"
        echo "🎯 Training Mode: $TRAINING_MODE"
        echo "🎯 Exec Mode: $EXEC_MODE (mapped)"
        echo "🤖 Models trained: ${{ steps.training.outputs.models-trained }}"
        echo "📊 Dataset: $DATASET"
        echo ""
        
        if [ "${{ steps.training.outputs.sentiment-updated }}" = "true" ]; then
          echo "✅ Sentiment Model:"
          echo "  😊 URL: https://huggingface.co/$SENTIMENT_MODEL"
          echo "  🏷️ Labels: positive, negative, neutral"
        fi
        
        if [ "${{ steps.training.outputs.importance-updated }}" = "true" ]; then
          echo "✅ Importance Model:"
          echo "  🎯 URL: https://huggingface.co/$IMPORTANCE_MODEL"
          echo "  🏷️ Labels: générale, importante, critique"
        fi
        
        if [ "${{ steps.training.outputs.correlations-updated }}" = "true" ]; then
          echo "✅ Correlations Model:"
          echo "  🌐 URL: https://huggingface.co/Bencode92/tradepulse-finbert-correlations"
          echo "  🏷️ Type: Multi-label (133 commodities)"
        fi
        
        echo ""
        echo "🔧 PROBLÈMES RÉSOLUS:"
        echo "  ❌ Avant: ImportError: No module named 'config'"
        echo "  ❌ Avant: HF_TOKEN non accessible"
        echo "  ✅ Maintenant: PYTHONPATH=${{ github.workspace }}"
        echo "  ✅ Maintenant: config.correlation_mapping accessible"
        echo "  ✅ Maintenant: HF_TOKEN propagé partout"
        echo ""
        echo "💡 Solution appliquée:"
        echo "  🔧 Job-level env: PYTHONPATH + HF_TOKEN"
        echo "  🔧 Disponible dans tous les steps"
        echo "  🔧 Plus besoin de os.chdir()"
        echo ""
        if [ "$EXEC_MODE" = "production" ]; then
          echo "🔗 Vérifiez HuggingFace:"
          echo "  🔗 https://huggingface.co/$SENTIMENT_MODEL"
          echo "  🔗 https://huggingface.co/$IMPORTANCE_MODEL"
          echo "  🔗 https://huggingface.co/Bencode92/tradepulse-finbert-correlations"
          echo "  ↳ Devrait afficher 'Updated a few seconds ago'"
        fi
        echo ""
        echo "🔄 Next commit will work without import errors!"

    - name: 📤 Upload Training Artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: full-fix-training-${{ github.run_id }}
        path: |
          models/**
          training_strategy.json
          pre_training_validation.json
          finetune.log
        retention-days: 30
