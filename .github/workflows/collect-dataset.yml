name: 📰 Daily News Dataset Collection

on:
  # Collecte quotidienne à 06:00 UTC (08:00 CET)
  schedule:
    - cron: '0 6 * * *'
  
  # Collecte manuelle
  workflow_dispatch:
    inputs:
      source:
        description: 'Source de données'
        required: true
        default: 'mixed'
        type: choice
        options:
          - 'mixed'
          - 'rss'
          - 'newsapi'
          - 'placeholder'
      
      count:
        description: "Nombre d'échantillons à collecter"
        required: true
        default: '40'
        type: string
      
      days:
        description: 'Fenêtre temporelle en jours'
        required: true
        default: '3'
        type: string
      
      force_commit:
        description: 'Forcer le commit même si aucun changement'
        required: false
        default: false
        type: boolean
      
      include_time:
        description: 'Inclure heure dans le nom de fichier'
        required: false
        default: false
        type: boolean

jobs:
  collect-and-commit:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    # Permissions explicites pour écriture
    permissions:
      contents: write
      issues: write
    
    outputs:
      dataset-created: ${{ steps.collect.outputs.dataset-created }}
      dataset-path: ${{ steps.collect.outputs.dataset-path }}
      sample-count: ${{ steps.collect.outputs.sample-count }}
    
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 1
        token: ${{ secrets.GITHUB_TOKEN }}
        persist-credentials: true

    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'

    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        
        # Dépendances de base
        pip install pandas>=2.0.0
        
        # Dépendances pour collecte avancée + ML
        pip install feedparser requests transformers torch || echo "ML dependencies failed, will use basic labeling"
        
        echo "✅ Dependencies installed"

    - name: 🔧 Set Environment Variables with Timezone
      run: |
        # Configurer le timezone Europe/Paris
        export TZ="Europe/Paris"
        
        # Déterminer les paramètres selon le mode de déclenchement
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "SOURCE=${{ github.event.inputs.source }}" >> $GITHUB_ENV
          echo "COUNT=${{ github.event.inputs.count }}" >> $GITHUB_ENV
          echo "DAYS=${{ github.event.inputs.days }}" >> $GITHUB_ENV
          echo "FORCE_COMMIT=${{ github.event.inputs.force_commit }}" >> $GITHUB_ENV
          echo "INCLUDE_TIME=${{ github.event.inputs.include_time }}" >> $GITHUB_ENV
        else
          # Déclenchement automatique (cron) - PARAMÈTRES OPTIMISÉS
          echo "SOURCE=mixed" >> $GITHUB_ENV
          echo "COUNT=40" >> $GITHUB_ENV
          echo "DAYS=3" >> $GITHUB_ENV
          echo "FORCE_COMMIT=false" >> $GITHUB_ENV
          echo "INCLUDE_TIME=false" >> $GITHUB_ENV
        fi
        
        # Générer le nom de fichier avec la bonne timezone
        if [ "$INCLUDE_TIME" = "true" ]; then
          # Avec suffixe horaire pour éviter les collisions
          DATASET_SUFFIX=$(TZ="Europe/Paris" date '+%Y%m%d_%H%M')
        else
          # Date uniquement en timezone Paris
          DATASET_SUFFIX=$(TZ="Europe/Paris" date '+%Y%m%d')
        fi
        
        echo "DATASET_SUFFIX=$DATASET_SUFFIX" >> $GITHUB_ENV
        echo "TZ=Europe/Paris" >> $GITHUB_ENV
        
        echo "🔧 Configuration set:"
        echo "  Timezone: Europe/Paris"
        echo "  Current time: $(TZ='Europe/Paris' date)"
        echo "  Dataset suffix: $DATASET_SUFFIX"
        echo "  Source: $SOURCE"
        echo "  Count: $COUNT" 
        echo "  Days: $DAYS"
        echo "  Force commit: $FORCE_COMMIT"
        echo "  Include time: $INCLUDE_TIME"

    - name: 📰 Collect News Dataset with ML Labeling
      id: collect
      env:
        NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
      run: |
        set -euo pipefail
        
        echo "📰 Collecte des actualités financières + ML Labeling..."
        echo "🔧 Paramètres: Source=$SOURCE, Count=$COUNT, Days=$DAYS"
        echo "🕐 Timezone: $TZ ($(date))"
        
        # Utiliser le suffixe généré pour le nom de fichier
        DATASET_FILE="datasets/news_${DATASET_SUFFIX}.csv"
        
        # NOUVEAU : ML Labeling activé automatiquement pour sources réelles
        if [ "$SOURCE" = "mixed" ] || [ "$SOURCE" = "rss" ]; then
          echo "🤖 ML Labeling automatique activé pour source: $SOURCE"
          EXTRA_ARGS="--auto-label --ml-model fallback --confidence-threshold 0.75"
        else
          echo "📝 Mode labeling basique pour source: $SOURCE"
          EXTRA_ARGS=""
        fi
        
        # Collecte avec paramètres optimisés
        if python scripts/collect_news.py \
            --source "$SOURCE" \
            --count "$COUNT" \
            --days "$DAYS" \
            --output "$DATASET_FILE" \
            --newsapi-key "${NEWSAPI_KEY:-}" \
            $EXTRA_ARGS; then
          
          echo "✅ Collecte réussie"
          echo "📁 Fichier cible: $DATASET_FILE"
          
          if [ -f "$DATASET_FILE" ]; then
            SAMPLE_COUNT=$(tail -n +2 "$DATASET_FILE" | wc -l)
            echo "📊 Dataset créé: $DATASET_FILE ($SAMPLE_COUNT échantillons)"
            
            # Vérifier si un fichier JSON de métadonnées a été créé
            JSON_FILE="${DATASET_FILE%.csv}.json"
            if [ -f "$JSON_FILE" ]; then
              echo "📁 Métadonnées créées: $JSON_FILE"
              
              # Afficher infos ML si disponibles
              if command -v jq >/dev/null 2>&1; then
                echo "🤖 Informations ML:"
                jq -r 'if .auto_labeling_enabled then "  ML activé: " + (.ml_model_used // "N/A") + " (confiance: " + (.confidence_threshold // 0 | tostring) + ")" else "  ML désactivé" end' "$JSON_FILE" 2>/dev/null || echo "  (infos ML non disponibles)"
                
                echo "📊 Distribution labels:"
                jq -r '.label_distribution | to_entries[] | "  " + .key + ": " + (.value | tostring)' "$JSON_FILE" 2>/dev/null || echo "  (distribution non disponible)"
              fi
            fi
            
            echo "dataset-created=true" >> $GITHUB_OUTPUT
            echo "dataset-path=$DATASET_FILE" >> $GITHUB_OUTPUT
            echo "sample-count=$SAMPLE_COUNT" >> $GITHUB_OUTPUT
          else
            echo "❌ Dataset non trouvé après collecte: $DATASET_FILE"
            exit 1
          fi
        else
          echo "❌ Échec de la collecte"
          echo "💡 Solutions possibles:"
          echo "  - Vérifier la connexion internet"
          echo "  - Tester avec 'placeholder' si RSS/NewsAPI échouent"
          echo "  - Vérifier la clé NEWSAPI_KEY pour sources newsapi/mixed"
          echo "  - Installer les dépendances ML: pip install transformers torch"
          exit 1
        fi

    - name: 🔍 Validate Dataset Quality
      if: steps.collect.outputs.dataset-created == 'true'
      run: |
        DATASET_PATH="${{ steps.collect.outputs.dataset-path }}"
        
        echo "🔍 Validation du dataset: $DATASET_PATH"
        
        # Utiliser notre script de validation si disponible
        if [ -f "scripts/validate_dataset.py" ]; then
          if python scripts/validate_dataset.py "$DATASET_PATH"; then
            echo "✅ Validation réussie"
          else
            echo "❌ Dataset invalide - arrêt du processus"
            echo "💡 Vérifiez la qualité des données collectées"
            exit 1
          fi
        else
          echo "⚠️ Script de validation non trouvé, validation basique..."
          
          # Validation basique
          if [ ! -s "$DATASET_PATH" ]; then
            echo "❌ Dataset vide"
            exit 1
          fi
          
          # Vérifier l'en-tête
          if ! head -1 "$DATASET_PATH" | grep -q "text,label"; then
            echo "❌ En-tête CSV incorrect"
            exit 1
          fi
          
          echo "✅ Validation basique OK"
        fi

    - name: 📊 Dataset Statistics (AMÉLIORÉ)
      if: steps.collect.outputs.dataset-created == 'true'
      run: |
        DATASET_PATH="${{ steps.collect.outputs.dataset-path }}"
        
        echo "📊 Statistiques du dataset (avec ML):"
        echo "====================================="
        
        # Statistiques CSV basiques
        TOTAL_LINES=$(wc -l < "$DATASET_PATH")
        DATA_LINES=$((TOTAL_LINES - 1))  # Exclure l'en-tête
        
        echo "📄 Fichier: $DATASET_PATH"
        echo "📊 Échantillons: $DATA_LINES"
        echo "💾 Taille: $(du -h "$DATASET_PATH" | cut -f1)"
        echo "🕐 Créé: $(TZ='Europe/Paris' date)"
        echo "🔧 Source: $SOURCE (fenêtre: $DAYS jours)"
        
        # Vérifier le cache de déduplication
        CACHE_FILE="datasets/.article_cache.json"
        if [ -f "$CACHE_FILE" ]; then
          if command -v jq >/dev/null 2>&1; then
            CACHE_SIZE=$(jq '.articles | length' "$CACHE_FILE" 2>/dev/null || echo "?")
            echo "🗄️ Cache déduplication: $CACHE_SIZE articles connus"
          else
            echo "🗄️ Cache déduplication: présent"
          fi
        fi
        
        # Distribution des labels avec pourcentages
        echo ""
        echo "🏷️ Distribution des labels:"
        tail -n +2 "$DATASET_PATH" | cut -d',' -f2 | sort | uniq -c | while read count label; do
          if command -v bc >/dev/null 2>&1; then
            percentage=$(echo "scale=1; $count * 100 / $DATA_LINES" | bc -l)
            echo "  $label: $count ($percentage%)"
          else
            echo "  $label: $count"
          fi
        done
        
        # Vérifier la qualité (pas de doublons "Sample X:")
        DUPLICATES=$(tail -n +2 "$DATASET_PATH" | cut -d',' -f1 | grep -c "^Sample [0-9]*:" || echo "0")
        if [ "$DUPLICATES" -gt 0 ]; then
          echo "⚠️ Doublons détectés: $DUPLICATES articles 'Sample X:'"
        else
          echo "✅ Aucun doublon 'Sample X:' détecté"
        fi
        
        # Aperçu du contenu (sans headers pour éviter confusion)
        echo ""
        echo "👀 Aperçu du contenu:"
        tail -n +2 "$DATASET_PATH" | head -3

    - name: 📝 Smart Commit and Push
      if: steps.collect.outputs.dataset-created == 'true'
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        set -euo pipefail
        
        # Configuration Git
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
        
        DATASET_PATH="${{ steps.collect.outputs.dataset-path }}"
        SAMPLE_COUNT="${{ steps.collect.outputs.sample-count }}"
        TODAY=$(TZ='Europe/Paris' date '+%Y-%m-%d')
        FULL_TIMESTAMP=$(TZ='Europe/Paris' date '+%Y-%m-%d %H:%M:%S %Z')
        
        # Diagnostic AVANT git add
        echo "🔍 Diagnostic Git AVANT git add:"
        echo "  Working directory status:"
        git status --porcelain | head -10 || true
        echo "  Dataset file exists: $([ -f "$DATASET_PATH" ] && echo "✅ YES" || echo "❌ NO")"
        if [ -f "$DATASET_PATH" ]; then
          echo "  Dataset file size: $(wc -l < "$DATASET_PATH") lines"
        fi
        
        # Ajouter seulement le fichier CSV (pas de JSON par défaut)
        echo "🔧 Executing: git add $DATASET_PATH"
        git add "$DATASET_PATH"
        
        # Ajouter le cache de déduplication s'il existe
        if [ -f "datasets/.article_cache.json" ]; then
          git add datasets/.article_cache.json
          echo "🗄️ Cache de déduplication ajouté"
        fi
        
        # Diagnostic APRÈS git add
        echo ""
        echo "🔍 Diagnostic Git APRÈS git add:"
        echo "  Staged files:"
        git diff --staged --name-only | head -10 || true
        
        # Vérifier s'il y a des changements
        HAS_CHANGES=false
        if ! git diff --staged --quiet; then
          HAS_CHANGES=true
          echo "✅ Changements détectés dans l'index Git"
          echo "📋 Fichiers à committer:"
          git diff --staged --name-only || true
        else
          echo "⚠️ AUCUN changement détecté dans l'index Git"
        fi
        
        # Décider du commit selon la logique
        if [ "$HAS_CHANGES" = "true" ]; then
          # Commit normal avec infos ML
          COMMIT_MSG=$(cat <<EOF
        📰 Add daily news dataset with ML labeling ($TODAY)

        📊 Dataset details:
        - File: $DATASET_PATH
        - Samples: $SAMPLE_COUNT
        - Source: $SOURCE (fenêtre: $DAYS jours)
        - Generated: $FULL_TIMESTAMP
        - ML Labeling: $([ "$SOURCE" = "mixed" ] || [ "$SOURCE" = "rss" ] && echo "✅ Enabled (FinBERT)" || echo "❌ Disabled")

        🤖 Auto-generated by Daily News Collection workflow v2
        EOF
        )
          
          git commit -m "$COMMIT_MSG"
          echo "✅ Commit effectué avec changements"
          
        elif [ "$FORCE_COMMIT" = "true" ]; then
          # Commit forcé
          COMMIT_MSG="🔄 Forced dataset update ($TODAY) - regenerated"
          git commit -m "$COMMIT_MSG" --allow-empty
          echo "🔄 Commit forcé effectué"
          
        else
          # Pas de changements
          echo "🔄 Pas de changements et force_commit=false → skip commit"
          echo "💡 Le dataset existe déjà et est identique"
          echo "✅ Workflow terminé avec succès (pas de commit nécessaire)"
          exit 0
        fi
        
        # Push avec authentification
        echo "🔧 Configuration de l'authentification Git..."
        git remote set-url origin "https://x-access-token:${GITHUB_TOKEN}@github.com/${{ github.repository }}.git"
        
        echo "🚀 Pushing to repository..."
        git push
        
        echo "✅ Dataset committed and pushed successfully"
        echo "🔄 This will trigger validation and fine-tuning workflows"

    - name: 📝 Create Summary Comment (ML INFO)
      if: always() && github.event_name == 'workflow_dispatch'
      uses: actions/github-script@v7
      with:
        script: |
          const status = '${{ job.status }}';
          const datasetCreated = '${{ steps.collect.outputs.dataset-created }}' === 'true';
          const datasetPath = '${{ steps.collect.outputs.dataset-path }}';
          const sampleCount = '${{ steps.collect.outputs.sample-count }}';
          const source = '${{ env.SOURCE }}';
          const days = '${{ env.DAYS }}';
          const runUrl = `${context.payload.repository.html_url}/actions/runs/${context.runId}`;
          
          let emoji, statusText, message;
          
          if (status === 'success' && datasetCreated) {
            emoji = '✅';
            statusText = 'SUCCESS';
            
            const mlEnabled = source === 'mixed' || source === 'rss';
            
            message = `🎉 Dataset créé avec succès + ML Labeling !
            
            **📊 Détails du dataset:**
            - 📄 Fichier: \`${datasetPath}\`
            - 📊 Échantillons: ${sampleCount}
            - 🔗 Source: ${source}
            - 📅 Fenêtre temporelle: ${days} jours
            - 🤖 ML Labeling: ${mlEnabled ? '✅ Activé (FinBERT)' : '❌ Désactivé'}
            - 🕐 Timezone: Europe/Paris
            
            **🔥 Améliorations appliquées:**
            - ✅ Déduplication améliorée (plus de "Sample X:" en doublon)
            - ✅ ML Labeling automatique pour sources réelles
            - ✅ Fallback intelligent si RSS/API échouent
            - ✅ Cache persistant pour éviter répétitions
            - ✅ CSV simplifié (text, label) compatible
            
            **🔄 Prochaines étapes:**
            1. 🔍 Validation automatique déclenchée
            2. 🤖 Fine-tuning si validation OK
            3. 🚀 Modèle disponible sur HuggingFace`;
          } else {
            emoji = '❌';
            statusText = 'FAILED';
            message = `🚨 Échec de la collecte de dataset.
            
            **⚠️ Problème détecté:**
            - Status: ${status}
            - Dataset créé: ${datasetCreated}
            
            **🔧 Actions suggérées:**
            1. Vérifier les logs du workflow
            2. Tester localement: \`python scripts/collect_news.py --source mixed --count 30\`
            3. Vérifier dépendances: \`pip install feedparser requests transformers torch\`
            4. Test fallback: \`python scripts/collect_news.py --source placeholder\``;
          }
          
          const commentBody = `## ${emoji} Daily News Collection + ML Labeling - ${statusText}
          
          ${message}
          
          ### 🔗 Liens utiles
          - 📊 [Workflow complet](${runUrl})
          - 🤖 Script ML: \`scripts/collect_news.py\` (version améliorée)
          - 📖 [Guide des datasets](DATASET_WORKFLOW.md)
          
          ---
          *🤖 Collecte automatique avec ML Labeling FinBERT activé*
          `;
          
          console.log('Workflow summary:', commentBody);

  # Job optionnel pour notifier en cas d'échec
  notify-failure:
    runs-on: ubuntu-latest
    needs: collect-and-commit
    if: failure() && github.event_name == 'schedule'
    
    permissions:
      issues: write
    
    steps:
    - name: 🚨 Notify Collection Failure
      uses: actions/github-script@v7
      with:
        script: |
          const issueBody = `# 🚨 Échec de la collecte quotidienne + ML

          La collecte automatique avec ML labeling du ${new Date().toISOString().split('T')[0]} a échoué.

          ## 📊 Détails
          - **Workflow:** Daily News Collection + ML Labeling
          - **Trigger:** Scheduled (cron)
          - **Timestamp:** ${new Date().toISOString()}
          - **Run ID:** ${context.runId}

          ## 🔗 Liens
          - [Workflow failed](${context.payload.repository.html_url}/actions/runs/${context.runId})
          - [Script collecte ML](${context.payload.repository.html_url}/blob/main/scripts/collect_news.py)

          ## 🔧 Actions suggérées
          1. Vérifier les logs du workflow
          2. Tester collecte + ML: \`python scripts/collect_news.py --source mixed --auto-label\`
          3. Vérifier dépendances ML: \`pip install transformers torch\`
          4. Test sans ML: \`python scripts/collect_news.py --source mixed\`
          5. Vérifier sources RSS/NewsAPI

          ## 🤖 Test ML Local
          \`\`\`bash
          # Test complet avec ML
          python scripts/collect_news.py --source mixed --count 20 --auto-label --ml-model fallback
          
          # Test simple sans ML
          python scripts/collect_news.py --source rss --count 15
          \`\`\`

          ---
          *Issue créée automatiquement par le système de monitoring ML*
          `;

          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `🚨 Échec collecte ML ${new Date().toISOString().split('T')[0]}`,
            body: issueBody,
            labels: ['bug', 'automation', 'dataset-collection', 'ml-labeling']
          });
