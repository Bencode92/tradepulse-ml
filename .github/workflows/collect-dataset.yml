name: üì∞ Daily News Dataset Collection

on:
  # Collecte quotidienne √† 06:00 UTC (08:00 CET)
  schedule:
    - cron: '0 6 * * *'
  
  # Collecte manuelle
  workflow_dispatch:
    inputs:
      source:
        description: 'Source de donn√©es'
        required: true
        default: 'mixed'
        type: choice
        options:
          - 'mixed'
          - 'rss'
          - 'newsapi'
          - 'placeholder'
      
      count:
        description: "Nombre d'√©chantillons √† collecter"
        required: true
        default: '40'
        type: string
      
      days:
        description: 'Fen√™tre temporelle en jours'
        required: true
        default: '3'
        type: string
      
      force_commit:
        description: 'Forcer le commit m√™me si aucun changement'
        required: false
        default: false
        type: boolean
      
      include_time:
        description: 'Inclure heure dans le nom de fichier'
        required: false
        default: false
        type: boolean

jobs:
  collect-and-commit:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    # Permissions explicites pour √©criture
    permissions:
      contents: write
      issues: write
    
    outputs:
      dataset-created: ${{ steps.collect.outputs.dataset-created }}
      dataset-path: ${{ steps.collect.outputs.dataset-path }}
      sample-count: ${{ steps.collect.outputs.sample-count }}
    
    steps:
    - name: üì• Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 1
        token: ${{ secrets.GITHUB_TOKEN }}
        persist-credentials: true

    - name: üêç Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'

    - name: üì¶ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        
        # D√©pendances de base
        pip install pandas>=2.0.0
        
        # D√©pendances pour collecte avanc√©e (NOUVEAU)
        pip install feedparser requests || echo "Optional dependencies failed, will use placeholder"
        
        echo "‚úÖ Dependencies installed"

    - name: üîß Set Environment Variables with Timezone
      run: |
        # Configurer le timezone Europe/Paris
        export TZ="Europe/Paris"
        
        # D√©terminer les param√®tres selon le mode de d√©clenchement
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "SOURCE=${{ github.event.inputs.source }}" >> $GITHUB_ENV
          echo "COUNT=${{ github.event.inputs.count }}" >> $GITHUB_ENV
          echo "DAYS=${{ github.event.inputs.days }}" >> $GITHUB_ENV
          echo "FORCE_COMMIT=${{ github.event.inputs.force_commit }}" >> $GITHUB_ENV
          echo "INCLUDE_TIME=${{ github.event.inputs.include_time }}" >> $GITHUB_ENV
        else
          # D√©clenchement automatique (cron) - NOUVEAUX D√âFAUTS OPTIMIS√âS
          echo "SOURCE=mixed" >> $GITHUB_ENV
          echo "COUNT=40" >> $GITHUB_ENV
          echo "DAYS=3" >> $GITHUB_ENV
          echo "FORCE_COMMIT=false" >> $GITHUB_ENV
          echo "INCLUDE_TIME=false" >> $GITHUB_ENV
        fi
        
        # G√©n√©rer le nom de fichier avec la bonne timezone
        if [ "$INCLUDE_TIME" = "true" ]; then
          # Avec suffixe horaire pour √©viter les collisions
          DATASET_SUFFIX=$(TZ="Europe/Paris" date '+%Y%m%d_%H%M')
        else
          # Date uniquement en timezone Paris
          DATASET_SUFFIX=$(TZ="Europe/Paris" date '+%Y%m%d')
        fi
        
        echo "DATASET_SUFFIX=$DATASET_SUFFIX" >> $GITHUB_ENV
        echo "TZ=Europe/Paris" >> $GITHUB_ENV
        
        echo "üîß Configuration set:"
        echo "  Timezone: Europe/Paris"
        echo "  Current time: $(TZ='Europe/Paris' date)"
        echo "  Dataset suffix: $DATASET_SUFFIX"
        echo "  Source: $SOURCE"
        echo "  Count: $COUNT" 
        echo "  Days: $DAYS"
        echo "  Force commit: $FORCE_COMMIT"
        echo "  Include time: $INCLUDE_TIME"

    - name: üì∞ Collect News Dataset (NOUVEAU - Version Avanc√©e)
      id: collect
      env:
        NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
      run: |
        set -euo pipefail
        
        echo "üì∞ Collecte des actualit√©s financi√®res (Version Avanc√©e)..."
        echo "üîß Param√®tres: Source=$SOURCE, Count=$COUNT, Days=$DAYS"
        echo "üïê Timezone: $TZ ($(date))"
        
        # Utiliser le suffixe g√©n√©r√© pour le nom de fichier
        DATASET_FILE="datasets/news_${DATASET_SUFFIX}.csv"
        
        # NOUVEAU : Appel avec param√®tre --days
        if python scripts/collect_news.py --source "$SOURCE" --count "$COUNT" --days "$DAYS" --output "$DATASET_FILE"; then
          echo "‚úÖ Collecte r√©ussie"
          echo "üìÅ Fichier cible: $DATASET_FILE"
          
          if [ -f "$DATASET_FILE" ]; then
            SAMPLE_COUNT=$(tail -n +2 "$DATASET_FILE" | wc -l)
            echo "üìä Dataset cr√©√©: $DATASET_FILE ($SAMPLE_COUNT √©chantillons)"
            
            # V√©rifier si un fichier JSON de m√©tadonn√©es a √©t√© cr√©√©
            JSON_FILE="${DATASET_FILE%.csv}.json"
            if [ -f "$JSON_FILE" ]; then
              echo "üìÅ M√©tadonn√©es cr√©√©es: $JSON_FILE"
              
              # Afficher les m√©tadonn√©es si possible (FIX: √©viter broken pipe)
              if command -v jq >/dev/null 2>&1; then
                echo "üìä M√©tadonn√©es du dataset:"
                # FIX: Utiliser --compact-output et √©viter le pipe vers head
                jq --compact-output '. | {filename, created_at, article_count, label_distribution, cache_size}' "$JSON_FILE" 2>/dev/null || echo "M√©tadonn√©es disponibles dans $JSON_FILE"
              fi
            fi
            
            echo "dataset-created=true" >> $GITHUB_OUTPUT
            echo "dataset-path=$DATASET_FILE" >> $GITHUB_OUTPUT
            echo "sample-count=$SAMPLE_COUNT" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Dataset non trouv√© apr√®s collecte: $DATASET_FILE"
            exit 1
          fi
        else
          echo "‚ùå √âchec de la collecte"
          echo "üí° Solutions possibles:"
          echo "  - V√©rifier la connexion internet"
          echo "  - Tester avec 'placeholder' si RSS/NewsAPI √©chouent"
          echo "  - V√©rifier la cl√© NEWSAPI_KEY pour sources newsapi/mixed"
          exit 1
        fi

    - name: üîç Validate Dataset Quality
      if: steps.collect.outputs.dataset-created == 'true'
      run: |
        DATASET_PATH="${{ steps.collect.outputs.dataset-path }}"
        
        echo "üîç Validation du dataset: $DATASET_PATH"
        
        # Utiliser notre script de validation si disponible
        if [ -f "scripts/validate_dataset.py" ]; then
          if python scripts/validate_dataset.py "$DATASET_PATH"; then
            echo "‚úÖ Validation r√©ussie"
          else
            echo "‚ùå Dataset invalide - arr√™t du processus"
            echo "üí° V√©rifiez la qualit√© des donn√©es collect√©es"
            exit 1
          fi
        else
          echo "‚ö†Ô∏è Script de validation non trouv√©, validation basique..."
          
          # Validation basique
          if [ ! -s "$DATASET_PATH" ]; then
            echo "‚ùå Dataset vide"
            exit 1
          fi
          
          # V√©rifier l'en-t√™te
          if ! head -1 "$DATASET_PATH" | grep -q "text,label"; then
            echo "‚ùå En-t√™te CSV incorrect"
            exit 1
          fi
          
          echo "‚úÖ Validation basique OK"
        fi

    - name: üìä Dataset Statistics (AM√âLIOR√â)
      if: steps.collect.outputs.dataset-created == 'true'
      run: |
        DATASET_PATH="${{ steps.collect.outputs.dataset-path }}"
        
        echo "üìä Statistiques du dataset (Version Avanc√©e):"
        echo "============================================="
        
        # Statistiques CSV basiques
        TOTAL_LINES=$(wc -l < "$DATASET_PATH")
        DATA_LINES=$((TOTAL_LINES - 1))  # Exclure l'en-t√™te
        
        echo "üìÑ Fichier: $DATASET_PATH"
        echo "üìä √âchantillons: $DATA_LINES"
        echo "üíæ Taille: $(du -h "$DATASET_PATH" | cut -f1)"
        echo "üïê Cr√©√©: $(TZ='Europe/Paris' date)"
        echo "üîß Source: $SOURCE (fen√™tre: $DAYS jours)"
        
        # V√©rifier le cache de d√©duplication (FIX: plus robuste)
        CACHE_FILE="datasets/.article_cache.json"
        if [ -f "$CACHE_FILE" ]; then
          if command -v python3 >/dev/null 2>&1; then
            CACHE_SIZE=$(python3 -c "
import json
try:
    with open('$CACHE_FILE', 'r') as f:
        data = json.load(f)
        print(len(data.get('articles', [])))
except:
    print('?')
" 2>/dev/null || echo "?")
            echo "üóÑÔ∏è Cache d√©duplication: $CACHE_SIZE articles connus"
          else
            echo "üóÑÔ∏è Cache d√©duplication: pr√©sent"
          fi
        fi
        
        # Distribution des labels si bc est disponible
        echo ""
        echo "üè∑Ô∏è Distribution des labels:"
        tail -n +2 "$DATASET_PATH" | cut -d',' -f2 | sort | uniq -c | while read count label; do
          if command -v bc >/dev/null 2>&1; then
            percentage=$(echo "scale=1; $count * 100 / $DATA_LINES" | bc -l)
            echo "  $label: $count ($percentage%)"
          else
            echo "  $label: $count"
          fi
        done
        
        # Aper√ßu du contenu
        echo ""
        echo "üëÄ Aper√ßu du contenu:"
        head -4 "$DATASET_PATH"

    - name: üìù Smart Commit and Push
      if: steps.collect.outputs.dataset-created == 'true'
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        set -euo pipefail
        
        # Configuration Git
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
        
        DATASET_PATH="${{ steps.collect.outputs.dataset-path }}"
        SAMPLE_COUNT="${{ steps.collect.outputs.sample-count }}"
        TODAY=$(TZ='Europe/Paris' date '+%Y-%m-%d')
        FULL_TIMESTAMP=$(TZ='Europe/Paris' date '+%Y-%m-%d %H:%M:%S %Z')
        
        # Diagnostic AVANT git add (safe commands with || true)
        echo "üîç Diagnostic Git AVANT git add:"
        echo "  Working directory status:"
        git status --porcelain | head -10 || true
        echo "  Dataset file exists: $([ -f "$DATASET_PATH" ] && echo "‚úÖ YES" || echo "‚ùå NO")"
        if [ -f "$DATASET_PATH" ]; then
          echo "  Dataset file size: $(wc -l < "$DATASET_PATH") lines"
        fi
        
        # Ajouter les fichiers datasets (inclut .json et .article_cache.json)
        echo "üîß Executing: git add datasets/"
        git add datasets/
        
        # Diagnostic APR√àS git add
        echo ""
        echo "üîç Diagnostic Git APR√àS git add:"
        echo "  Staged files (via git diff):"
        git diff --staged --name-only | head -10 || true
        echo "  Working directory status:"
        git status --porcelain | head -10 || true
        
        # V√©rifier s'il y a des changements - logique intelligente
        HAS_CHANGES=false
        if ! git diff --staged --quiet; then
          HAS_CHANGES=true
          echo "‚úÖ Changements d√©tect√©s dans l'index Git"
          echo "üìã Fichiers √† committer:"
          git diff --staged --name-only || true
        else
          echo "‚ö†Ô∏è AUCUN changement d√©tect√© dans l'index Git"
        fi
        
        # D√©cider du commit selon la logique
        if [ "$HAS_CHANGES" = "true" ]; then
          # Il y a des changements ‚Üí commit normal
          COMMIT_ACTION="normal"
          
          # Message am√©lior√© avec infos de la collecte avanc√©e
          COMMIT_MSG=$(cat <<EOF
        üì∞ Add daily financial news dataset ($TODAY)

        üìä Dataset details:
        - File: $DATASET_PATH
        - Samples: $SAMPLE_COUNT
        - Source: $SOURCE (fen√™tre: $DAYS jours)
        - Generated: $FULL_TIMESTAMP
        - Collection: Version avanc√©e avec d√©duplication

        üöÄ Nouveaut√©s: Cache d√©duplication, sources multiples, fen√™tre temporelle
        ü§ñ Auto-generated by Daily News Collection workflow
        EOF
        )
          
        elif [ "$FORCE_COMMIT" = "true" ]; then
          # Pas de changements mais commit forc√©
          COMMIT_ACTION="forced"
          
          COMMIT_MSG=$(cat <<EOF
        üîÑ Forced dataset update ($TODAY)

        üìä Dataset details:
        - File: $DATASET_PATH (regenerated)
        - Samples: $SAMPLE_COUNT
        - Source: $SOURCE (fen√™tre: $DAYS jours)
        - Generated: $FULL_TIMESTAMP

        ‚ö†Ô∏è Forced commit with --allow-empty (manual trigger)
        EOF
        )
          
        else
          # Pas de changements et pas de commit forc√© ‚Üí skip proprement
          echo "üîÑ Pas de changements et force_commit=false ‚Üí skip commit"
          echo "üí° Le dataset existe d√©j√† et est identique"
          echo "üõ†Ô∏è Pour forcer un commit: relancez avec force_commit=true"
          echo "‚úÖ Workflow termin√© avec succ√®s (pas de commit n√©cessaire)"
          exit 0
        fi
        
        # Ex√©cuter le commit selon l'action
        echo ""
        echo "üöÄ Action: commit $COMMIT_ACTION"
        
        if [ "$COMMIT_ACTION" = "forced" ]; then
          git commit -m "$COMMIT_MSG" --allow-empty
          echo "üîÑ Commit forc√© effectu√© (m√™me sans changements)"
        else
          git commit -m "$COMMIT_MSG"
          echo "‚úÖ Commit normal effectu√© avec changements"
        fi
        
        # Configurer l'URL remote avec le token pour le push
        echo "üîß Configuration de l'authentification Git..."
        git remote set-url origin "https://x-access-token:${GITHUB_TOKEN}@github.com/${{ github.repository }}.git"
        
        echo "üöÄ Pushing to repository avec authentification..."
        git push
        
        echo "‚úÖ Dataset committed and pushed successfully"
        echo "üîÑ This will trigger validation and fine-tuning workflows"

    - name: üìù Create Summary Comment (AM√âLIOR√â)
      if: always() && github.event_name == 'workflow_dispatch'
      uses: actions/github-script@v7
      with:
        script: |
          const status = '${{ job.status }}';
          const datasetCreated = '${{ steps.collect.outputs.dataset-created }}' === 'true';
          const datasetPath = '${{ steps.collect.outputs.dataset-path }}';
          const sampleCount = '${{ steps.collect.outputs.sample-count }}';
          const source = '${{ env.SOURCE }}';
          const days = '${{ env.DAYS }}';
          const datasetSuffix = '${{ env.DATASET_SUFFIX }}';
          const runUrl = `${context.payload.repository.html_url}/actions/runs/${context.runId}`;
          
          let emoji, statusText, message;
          
          if (status === 'success' && datasetCreated) {
            emoji = '‚úÖ';
            statusText = 'SUCCESS';
            message = `üéâ Dataset cr√©√© avec succ√®s (Version Avanc√©e) !
            
            **üìä D√©tails du dataset:**
            - üìÑ Fichier: \`${datasetPath}\`
            - üìä √âchantillons: ${sampleCount}
            - üîó Source: ${source}
            - üìÖ Fen√™tre temporelle: ${days} jours
            - üìÖ Suffixe: ${datasetSuffix}
            - üïê Timezone: Europe/Paris
            - üöÄ Timestamp: ${new Date().toISOString()}
            
            **üî• Nouvelles fonctionnalit√©s:**
            - ‚úÖ Cache de d√©duplication automatique
            - ‚úÖ 12 sources RSS au lieu de 4
            - ‚úÖ Pagination NewsAPI intelligente
            - ‚úÖ Mode mixte optimis√© (70% RSS + 30% NewsAPI)
            - ‚úÖ M√©tadonn√©es enrichies
            
            **üîÑ Prochaines √©tapes:**
            Le dataset a √©t√© committ√© et va d√©clencher automatiquement :
            1. üîç Validation qualit√© (Dataset Quality Gate)
            2. ü§ñ Fine-tuning automatique (si validation OK)
            3. üöÄ D√©ploiement sur HuggingFace (si configur√©)`;
          } else {
            emoji = '‚ùå';
            statusText = 'FAILED';
            message = `üö® √âchec de la collecte de dataset.
            
            **‚ö†Ô∏è Probl√®me d√©tect√©:**
            - Status: ${status}
            - Dataset cr√©√©: ${datasetCreated}
            
            **üîß Actions sugg√©r√©es:**
            1. V√©rifier les logs du workflow
            2. Tester la collecte localement: \`./run_advanced_daily.sh mixed 30 3\`
            3. V√©rifier les d√©pendances (feedparser, requests)
            4. Tester avec placeholder: \`python scripts/collect_news.py --source placeholder\`
            5. V√©rifier les r√®gles .gitignore`;
          }
          
          const commentBody = `## ${emoji} Daily News Collection (Version Avanc√©e) - ${statusText}
          
          ${message}
          
          ### üîó Liens utiles
          - üìä [Workflow complet](${runUrl})
          - üîß Script: \`scripts/collect_news.py\` (Version Avanc√©e)
          - üìñ [Guide des datasets](DATASET_WORKFLOW.md)
          - üöÄ Script express: \`./run_advanced_daily.sh\`
          
          ---
          *ü§ñ Rapport automatique de collecte quotidienne - Version Avanc√©e avec d√©duplication*
          `;
          
          console.log('Workflow summary:', commentBody);

  # Job optionnel pour notifier en cas d'√©chec
  notify-failure:
    runs-on: ubuntu-latest
    needs: collect-and-commit
    if: failure() && github.event_name == 'schedule'
    
    permissions:
      issues: write
    
    steps:
    - name: üö® Notify Collection Failure
      uses: actions/github-script@v7
      with:
        script: |
          const issueBody = `# üö® √âchec de la collecte quotidienne de dataset

          La collecte automatique de dataset du ${new Date().toISOString().split('T')[0]} a √©chou√©.

          ## üìä D√©tails
          - **Workflow:** Daily News Dataset Collection (Version Avanc√©e)
          - **Trigger:** Scheduled (cron)
          - **Timestamp:** ${new Date().toISOString()}
          - **Run ID:** ${context.runId}

          ## üîó Liens
          - [Workflow failed](${context.payload.repository.html_url}/actions/runs/${context.runId})
          - [Script de collecte](${context.payload.repository.html_url}/blob/main/scripts/collect_news.py)

          ## üîß Actions sugg√©r√©es
          1. V√©rifier les logs du workflow
          2. Tester la collecte manuellement: \`./run_advanced_daily.sh mixed 40 3\`
          3. V√©rifier la disponibilit√© des sources RSS/API
          4. Tester avec placeholder: \`python scripts/collect_news.py --source placeholder\`
          5. Lancer une collecte manuelle si n√©cessaire

          ## üí° Collecte manuelle
          Vous pouvez lancer une collecte manuelle via :
          **Actions ‚Üí Daily News Dataset Collection ‚Üí Run workflow**

          Nouvelles options disponibles :
          - **Source:** mixed (recommand√©), rss, newsapi, placeholder
          - **Count:** 40 (d√©faut optimis√©)
          - **Days:** 3 (fen√™tre temporelle en jours)

          ---
          *Issue cr√©√©e automatiquement par le syst√®me de monitoring*
          `;

          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `üö® √âchec collecte automatique dataset ${new Date().toISOString().split('T')[0]}`,
            body: issueBody,
            labels: ['bug', 'automation', 'dataset-collection']
          });
