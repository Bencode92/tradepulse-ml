#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TradePulse ‚Äì FinBERT Fine‚ÄëTuning Utility avec Apprentissage Incr√©mental
======================================================================

üöÄ NOUVEAU : Apprentissage Incr√©mental !
- Mode --incremental : Am√©liore un mod√®le existant au lieu de cr√©er un nouveau
- Validation automatique avant mise √† jour  
- Mod√®les production/d√©veloppement s√©par√©s
- Rollback automatique si d√©gradation

üéØ NOUVEAU : Support Importance !
- --target-column importance : Entra√Æne sur l'importance (critique/importante/g√©n√©rale)
- --target-column label : Entra√Æne sur le sentiment (positive/negative/neutral)

‚Ä¢  Charge un corpus (CSV/JSON) de textes financiers d√©j√† √©tiquet√©s
  en **positive / neutral / negative** ou **critique / importante / g√©n√©rale**.
‚Ä¢  D√©coupe automatiquement en train / validation (80 / 20 stratifi√©).
‚Ä¢  Tokenise, fine‚Äëtune et enregistre un FinBERT (ou autre mod√®le) d√©j√†
  pr√©sent sur HuggingFace Hub.
‚Ä¢  Produit un *training_report.json* + logs TensorBoard dans <output_dir>.

Usage Sentiment (existant):
---------------------------
$ python finetune.py \
    --dataset datasets/news_20250705.csv \
    --output_dir models/finbert-sentiment \
    --target-column label

üéØ NOUVEAU - Usage Importance:
-----------------------------
$ python finetune.py \
    --dataset datasets/news_20250705.csv \
    --output_dir models/finbert-importance \
    --target-column importance

üöÄ NOUVEAU - Usage Apprentissage Incr√©mental:
--------------------------------------------
# Mode production (am√©liore le mod√®le stable)
$ python finetune.py --incremental --mode production --dataset datasets/news_20250707.csv

# Mode d√©veloppement (am√©liore le mod√®le de dev)
$ python finetune.py --incremental --mode development --dataset datasets/news_20250707.csv

# Mode test (validation locale, pas de mise √† jour)
$ python finetune.py --incremental --mode test --dataset datasets/news_20250707.csv

Auto-s√©lection (existant, conserv√©):
-----------------------------------
$ python finetune.py --output_dir models/finbert-auto
  # Auto-d√©tecte le dernier dataset
"""
from __future__ import annotations

import argparse
import json
import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple
from collections import Counter

import numpy as np
import pandas as pd
import torch
from datasets import Dataset, DatasetDict
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.model_selection import train_test_split
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    EarlyStoppingCallback,
    EvalPrediction,
    Trainer,
    TrainingArguments,
    set_seed,
)

# üöÄ NOUVEAU : Configuration des mod√®les de production
MODELS_CONFIG = {
    "production": {
        "hf_id": "Bencode92/tradepulse-finbert-prod",          # Mod√®le PRODUCTION stable
        "description": "Mod√®le stable pour site TradePulse",
        "auto_update": True,  # Met √† jour automatiquement si am√©lioration
    },
    "development": {
        "hf_id": "Bencode92/tradepulse-finbert-dev",          # Mod√®le DEV pour tests
        "description": "Mod√®le de d√©veloppement et tests",
        "auto_update": False,  # Mise √† jour manuelle seulement
    }
}

# üöÄ NOUVEAU : Seuils de performance pour validation
PERFORMANCE_THRESHOLDS = {
    "min_accuracy": 0.70,      # Pr√©cision minimum acceptable (r√©duit pour petits datasets)
    "min_f1": 0.65,            # F1-score minimum (r√©duit pour petits datasets)
    "improvement_threshold": 0.02,  # Am√©lioration minimum pour mise √† jour (2%)
}

# Auto-s√©lection helper (existant, conserv√©)
try:
    from utils import get_date_from_filename, latest_dataset
    AUTOSEL = True
except ImportError:
    AUTOSEL = False

# ---------------------------------------------------------------------------
# Logging (existant, conserv√©)
# ---------------------------------------------------------------------------
LOG_FMT = "%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s"
logging.basicConfig(
    level=logging.INFO,
    format=LOG_FMT,
    handlers=[logging.FileHandler("finetune.log"), logging.StreamHandler()],
)
logger = logging.getLogger("tradepulse-finetune")


# ---------------------------------------------------------------------------
# Fine‚Äëtuner class (adapt√© pour supporter l'incr√©mental + importance)
# ---------------------------------------------------------------------------
class Finetuner:
    # üòä Labels pour sentiment (existant)
    SENTIMENT_LABEL_MAP: Dict[str, int] = {"negative": 0, "neutral": 1, "positive": 2}
    SENTIMENT_ID2LABEL: Dict[int, str] = {v: k for k, v in SENTIMENT_LABEL_MAP.items()}
    
    # üéØ NOUVEAU : Labels pour importance
    IMPORTANCE_LABEL_MAP: Dict[str, int] = {"g√©n√©rale": 0, "importante": 1, "critique": 2}
    IMPORTANCE_ID2LABEL: Dict[int, str] = {v: k for k, v in IMPORTANCE_LABEL_MAP.items()}

    def __init__(self, model_name: str, max_length: int, incremental_mode: bool = False, 
                 baseline_model: str = None, target_column: str = "label"):
        self.model_name = model_name
        self.max_length = max_length
        self.incremental_mode = incremental_mode
        self.baseline_model = baseline_model
        self.target_column = target_column  # üéØ NOUVEAU
        
        # üéØ NOUVEAU : S√©lection des labels selon la colonne cible
        if target_column == "importance":
            self.LABEL_MAP = self.IMPORTANCE_LABEL_MAP
            self.ID2LABEL = self.IMPORTANCE_ID2LABEL
            logger.info("üéØ Mode entra√Ænement : IMPORTANCE (critique/importante/g√©n√©rale)")
        else:
            self.LABEL_MAP = self.SENTIMENT_LABEL_MAP
            self.ID2LABEL = self.SENTIMENT_ID2LABEL
            logger.info("üòä Mode entra√Ænement : SENTIMENT (positive/negative/neutral)")
        
        if incremental_mode and baseline_model:
            # üöÄ NOUVEAU : Mode incr√©mental - charger mod√®le existant
            try:
                logger.info(f"üì• Tentative de chargement du mod√®le existant: {baseline_model}")
                self.tokenizer = AutoTokenizer.from_pretrained(baseline_model)
                self.model = AutoModelForSequenceClassification.from_pretrained(
                    baseline_model,
                    num_labels=3,
                    id2label=self.ID2LABEL,
                    label2id=self.LABEL_MAP,
                )
                logger.info(f"‚úÖ Mod√®le incr√©mental charg√©: {baseline_model}")
                self.model_name = baseline_model  # Update model name
            except Exception as e:
                # Fallback sur le mod√®le de base
                logger.warning(f"‚ö†Ô∏è Impossible de charger {baseline_model}: {e}")
                logger.info(f"üîÑ Fallback sur mod√®le de base: {model_name}")
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModelForSequenceClassification.from_pretrained(
                    model_name,
                    num_labels=3,
                    id2label=self.ID2LABEL,
                    label2id=self.LABEL_MAP,
                )
                self.baseline_model = model_name
        else:
            # Mode classique (existant, conserv√©)
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                num_labels=3,
                id2label=self.ID2LABEL,
                label2id=self.LABEL_MAP,
            )
            logger.info("‚úÖ Model & tokenizer loaded : %s", model_name)

    # -------------------------------------------------------------------
    # üöÄ NOUVEAU : M√©thodes d'√©valuation pour mode incr√©mental
    # -------------------------------------------------------------------
    def evaluate_on_test(self, test_dataset: Dataset) -> Dict[str, float]:
        """√âvalue le mod√®le sur un dataset de test"""
        # Tokenisation du dataset de test
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=self.max_length,
            )
        
        eval_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
        
        # √âvaluation avec Trainer
        eval_args = TrainingArguments(
            output_dir="./tmp_eval",
            per_device_eval_batch_size=32,
            logging_dir=None,
            report_to=[],  # Pas de logging externe
        )
        
        trainer = Trainer(
            model=self.model,
            args=eval_args,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            compute_metrics=self._metrics,
        )
        
        metrics = trainer.evaluate()
        
        # Nettoyage des m√©triques 
        clean_metrics = {
            "accuracy": metrics.get("eval_accuracy", 0.0),
            "f1": metrics.get("eval_f1", 0.0),
            "precision": metrics.get("eval_precision", 0.0),
            "recall": metrics.get("eval_recall", 0.0),
        }
        
        return clean_metrics

    def should_update_model(self, baseline_metrics: Dict[str, float], new_metrics: Dict[str, float], 
                           min_improvement: float = None) -> Tuple[bool, str]:
        """D√©termine si le nouveau mod√®le doit remplacer l'ancien"""
        
        min_improvement = min_improvement or PERFORMANCE_THRESHOLDS["improvement_threshold"]
        
        # V√©rification des seuils minimum
        if new_metrics["accuracy"] < PERFORMANCE_THRESHOLDS["min_accuracy"]:
            return False, f"Pr√©cision insuffisante: {new_metrics['accuracy']:.3f} < {PERFORMANCE_THRESHOLDS['min_accuracy']}"
        
        if new_metrics["f1"] < PERFORMANCE_THRESHOLDS["min_f1"]:
            return False, f"F1-score insuffisant: {new_metrics['f1']:.3f} < {PERFORMANCE_THRESHOLDS['min_f1']}"
        
        # V√©rification de l'am√©lioration
        accuracy_improvement = new_metrics["accuracy"] - baseline_metrics["accuracy"]
        f1_improvement = new_metrics["f1"] - baseline_metrics["f1"]
        
        if accuracy_improvement >= min_improvement or f1_improvement >= min_improvement:
            return True, f"Am√©lioration d√©tect√©e - Accuracy: +{accuracy_improvement:.3f}, F1: +{f1_improvement:.3f}"
        
        return False, f"Am√©lioration insuffisante - Accuracy: {accuracy_improvement:+.3f}, F1: {f1_improvement:+.3f} (min: {min_improvement})"

    def push_to_huggingface(self, model_path: Path, hf_model_id: str, commit_message: str = None):
        """Push le mod√®le vers HuggingFace Hub"""
        
        commit_message = commit_message or f"Apprentissage incr√©mental - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        
        try:
            # Chargement du mod√®le depuis le dossier local
            model = AutoModelForSequenceClassification.from_pretrained(model_path)
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # Push vers HuggingFace
            logger.info(f"üì§ Push vers HuggingFace: {hf_model_id}")
            
            model.push_to_hub(
                hf_model_id,
                commit_message=commit_message,
                private=False  # Ou True si vous voulez un repo priv√©
            )
            
            tokenizer.push_to_hub(
                hf_model_id,
                commit_message=commit_message,
                private=False
            )
            
            logger.info(f"‚úÖ Mod√®le push√© vers HuggingFace: https://huggingface.co/{hf_model_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du push: {e}")
            raise

    # -------------------------------------------------------------------
    # Data helpers (adapt√©s pour supporter le mode incr√©mental + petits datasets + importance)
    # -------------------------------------------------------------------
    def _load_raw(self, path: Path) -> List[Dict[str, str]]:
        if path.suffix.lower() == ".csv":
            return pd.read_csv(path).to_dict("records")
        if path.suffix.lower() == ".json":
            return json.loads(path.read_text("utf-8"))
        raise ValueError(f"Unsupported file type : {path}")

    def _standardise(self, rows: List[Dict[str, str]]) -> List[Dict[str, str]]:
        out: List[Dict[str, str]] = []
        for row in rows:
            text = row.get("text") or (
                f"{row.get('title', '')} {row.get('content', '')}"
            ).strip()
            
            # üéØ NOUVEAU : S√©lection de la colonne selon target_column
            if self.target_column == "importance":
                label = row.get("importance", "").lower()
            else:
                label = (
                    row.get("label")
                    or row.get("sentiment") 
                    or row.get("impact")
                    or ""
                ).lower()
            
            if not text or label not in self.LABEL_MAP:
                continue
            out.append({"text": text, "label": self.LABEL_MAP[label]})
        return out

    def _check_dataset_balance(self, data: List[Dict[str, str]]) -> bool:
        """V√©rifie si le dataset est suffisamment √©quilibr√© pour un split stratifi√©"""
        label_counts = Counter([d["label"] for d in data])
        
        logger.info(f"üìä Distribution des labels: {dict(label_counts)}")
        
        # V√©rifier que chaque classe a au moins 2 √©chantillons
        min_samples = min(label_counts.values()) if label_counts else 0
        
        if min_samples < 2:
            logger.warning(f"‚ö†Ô∏è Dataset d√©s√©quilibr√©: classe minimale = {min_samples} √©chantillon(s)")
            logger.warning(f"‚ö†Ô∏è Split stratifi√© impossible, utilisation d'un split simple")
            return False
        
        return True

    def load_dataset(self, path: Path) -> Tuple[DatasetDict, Dataset]:
        """Load & tokenise dataset, return HF DatasetDict + test dataset pour √©valuation incr√©mentale"""
        raw = self._load_raw(path)
        data = self._standardise(raw)
        if not data:
            raise RuntimeError("No usable samples detected in dataset !")
        logger.info("üìä %d samples after cleaning", len(data))

        # üîß CORRECTION : V√©rifier si on peut faire un split stratifi√©
        can_stratify = self._check_dataset_balance(data)
        
        if self.incremental_mode:
            # üöÄ NOUVEAU : Division en 3 parties pour l'apprentissage incr√©mental
            # 70% train, 20% validation, 10% test (pour √©valuation baseline)
            if len(data) >= 10 and can_stratify:
                # Split stratifi√© si possible
                train_val, test_data = train_test_split(
                    data,
                    test_size=0.1,
                    stratify=[d["label"] for d in data],
                    random_state=42,
                )
                
                train, val = train_test_split(
                    train_val,
                    test_size=0.25,  # 0.25 de 90% = 22.5% du total ‚âà 20%
                    stratify=[d["label"] for d in train_val] if self._check_dataset_balance(train_val) else None,
                    random_state=42,
                )
            else:
                # Split simple pour petits datasets
                logger.warning("‚ö†Ô∏è Dataset trop petit pour split optimal, utilisation de proportions adapt√©es")
                
                if len(data) >= 5:
                    # Au moins 5 √©chantillons : 1 pour test, reste pour train/val
                    test_data = data[:1]
                    train_val = data[1:]
                    
                    if len(train_val) >= 2:
                        train = train_val[:-1]
                        val = train_val[-1:]
                    else:
                        train = train_val
                        val = []
                else:
                    # Tr√®s petit dataset : tout en train, pas de validation
                    train = data
                    val = []
                    test_data = []
            
            logger.info(f"üìä Mode incr√©mental - Train: {len(train)}, Validation: {len(val)}, Test: {len(test_data)}")
        else:
            # Mode classique (existant, conserv√© mais avec gestion des petits datasets)
            if len(data) >= 4 and can_stratify:
                # Split stratifi√© si possible
                train, val = train_test_split(
                    data,
                    test_size=0.2,
                    stratify=[d["label"] for d in data],
                    random_state=42,
                )
            else:
                # Split simple pour petits datasets
                logger.warning("‚ö†Ô∏è Dataset trop petit ou d√©s√©quilibr√©, split simple sans stratification")
                split_idx = max(1, int(len(data) * 0.8))
                train = data[:split_idx]
                val = data[split_idx:] if len(data) > split_idx else []
            
            test_data = []  # Pas de dataset de test en mode classique
            logger.info(f"üìä Mode classique - Train: {len(train)}, Validation: {len(val)}")

        def tok(batch):
            return self.tokenizer(
                batch["text"],
                truncation=True,
                padding="max_length",
                max_length=self.max_length,
            )

        train_ds = Dataset.from_list(train).map(
            tok, batched=True, remove_columns=["text"]
        )
        
        # G√©rer le cas o√π il n'y a pas de validation
        if val:
            val_ds = Dataset.from_list(val).map(
                tok, batched=True, remove_columns=["text"]
            )
        else:
            # Utiliser une partie du train comme validation si pas de val
            logger.warning("‚ö†Ô∏è Pas de donn√©es de validation, utilisation d'une partie du train")
            val_ds = train_ds
        
        # Dataset de test (pour √©valuation baseline en mode incr√©mental)
        test_ds = Dataset.from_list(test_data) if test_data else None
        
        return DatasetDict(train=train_ds, validation=val_ds), test_ds

    # -------------------------------------------------------------------
    # Metrics (existant, conserv√©)
    # -------------------------------------------------------------------
    @staticmethod
    def _metrics(pred: EvalPrediction) -> Dict[str, float]:
        logits, labels = pred
        preds = np.argmax(logits, axis=1)
        prec, rec, f1, _ = precision_recall_fscore_support(
            labels, preds, average="weighted"
        )
        acc = accuracy_score(labels, preds)
        return {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1}

    # -------------------------------------------------------------------
    # Training (adapt√© pour supporter l'incr√©mental + petits datasets)
    # -------------------------------------------------------------------
    def train(self, ds: DatasetDict, args: argparse.Namespace, test_ds: Dataset = None):
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # üöÄ NOUVEAU : Nommage diff√©rent selon le mode
        if self.incremental_mode:
            run_name = f"incremental-{getattr(args, 'mode', 'test')}-{ts}"
        else:
            run_name = f"finbert-{self.target_column}-{ts}"  # üéØ Inclure target_column

        # üöÄ NOUVEAU : Arguments d'entra√Ænement adapt√©s pour l'incr√©mental + petits datasets
        if self.incremental_mode:
            # Param√®tres plus conservateurs pour l'apprentissage incr√©mental
            learning_rate = 1e-5  # Plus faible pour la stabilit√©
            epochs = min(2, args.epochs)  # Moins d'epochs pour √©viter l'overfitting
            batch_size = min(4, args.train_bs)  # Plus petit batch pour petits datasets
            patience = 1  # Arr√™t pr√©coce plus agressif
        else:
            # Param√®tres classiques adapt√©s pour petits datasets
            learning_rate = args.lr
            epochs = min(args.epochs, 5)  # Limiter les epochs pour petits datasets
            batch_size = min(8, args.train_bs)  # Adapter la taille de batch
            patience = 2

        targs = TrainingArguments(
            output_dir=args.output_dir,
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=min(batch_size, args.eval_bs),
            learning_rate=learning_rate,
            weight_decay=args.weight_decay,
            warmup_steps=min(args.warmup, len(ds["train"]) // 4),  # Adapter warmup
            evaluation_strategy="epoch" if len(ds["validation"]) > 0 else "no",
            save_strategy="epoch",
            load_best_model_at_end=len(ds["validation"]) > 0,
            metric_for_best_model="f1" if len(ds["validation"]) > 0 else None,
            greater_is_better=True,
            logging_dir=os.path.join(args.output_dir, "logs"),
            logging_steps=max(1, args.logging_steps // 10),  # Plus de logs pour petits datasets
            seed=args.seed,
            push_to_hub=False if self.incremental_mode else args.push,  # Gestion manuelle en mode incr√©mental
            hub_model_id=args.hub_id if (args.push and not self.incremental_mode) else None,
            report_to="tensorboard",
            save_total_limit=1,  # √âconomiser l'espace disque
            dataloader_drop_last=False,  # Ne pas ignorer les derniers √©chantillons
        )

        trainer = Trainer(
            model=self.model,
            args=targs,
            train_dataset=ds["train"],
            eval_dataset=ds["validation"] if len(ds["validation"]) > 0 else None,
            tokenizer=self.tokenizer,
            compute_metrics=self._metrics if len(ds["validation"]) > 0 else None,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)] if len(ds["validation"]) > 0 else [],
        )

        mode_info = f"incremental-{self.target_column}" if self.incremental_mode else f"classic-{self.target_column}"
        logger.info(f"üî• Start training for %d epochs (mode: %s)", epochs, mode_info)
        trainer.train()
        trainer.save_model()

        if len(ds["validation"]) > 0:
            eval_res = trainer.evaluate()
            logger.info(
                "‚úÖ Training complete ‚Äî F1: %.4f | Acc: %.4f",
                eval_res["eval_f1"],
                eval_res["eval_accuracy"],
            )
        else:
            logger.info("‚úÖ Training complete (no validation data)")
            eval_res = {"eval_f1": 0.0, "eval_accuracy": 0.0}

        # üöÄ NOUVEAU : √âvaluation sur le dataset de test pour mode incr√©mental
        test_metrics = None
        if test_ds is not None and self.incremental_mode and len(test_ds) > 0:
            logger.info("üìä √âvaluation sur dataset de test...")
            test_metrics = self.evaluate_on_test(test_ds)
            logger.info(f"üìä M√©triques test: {test_metrics}")

        # save a report (adapt√© pour mode incr√©mental + importance)
        report = {
            "model": self.model_name,
            "mode": "incremental" if self.incremental_mode else "classic",
            "target_column": self.target_column,  # üéØ NOUVEAU
            "label_mapping": dict(self.LABEL_MAP),  # üéØ NOUVEAU
            "epochs": epochs,
            "learning_rate": learning_rate,
            "validation_metrics": eval_res,
            "test_metrics": test_metrics,
            "timestamp": datetime.now().isoformat(),
        }
        
        # üöÄ NOUVEAU : Ajouter infos incr√©mentales si applicable
        if self.incremental_mode:
            report.update({
                "incremental_mode": getattr(args, 'mode', 'test'),
                "baseline_model": self.baseline_model,
            })

        Path(args.output_dir).mkdir(parents=True, exist_ok=True)
        
        # üöÄ NOUVEAU : Nom de rapport diff√©rent selon le mode
        report_name = "incremental_training_report.json" if self.incremental_mode else "training_report.json"
        
        with open(Path(args.output_dir, report_name), "w") as fh:
            json.dump(report, fh, indent=2)

        return test_metrics


# ---------------------------------------------------------------------------
# CLI (adapt√© pour supporter l'incr√©mental + importance)
# ---------------------------------------------------------------------------
def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        description="TradePulse FinBERT fine‚Äëtuning utility avec apprentissage incr√©mental et support importance"
    )
    p.add_argument(
        "--dataset",
        type=Path,
        help="Path to CSV/JSON dataset (auto-d√©tection si omis)",
    )
    p.add_argument(
        "--output_dir",
        required=True,
        type=Path,
        help="Where to save the model & logs",
    )
    
    # Arguments existants (conserv√©s)
    p.add_argument("--model_name", default="yiyanghkust/finbert-tone")
    p.add_argument("--epochs", type=int, default=3)
    p.add_argument("--lr", type=float, default=2e-5, help="Learning rate")
    p.add_argument("--train_bs", type=int, default=16, help="Train batch size")
    p.add_argument("--eval_bs", type=int, default=32, help="Eval batch size")
    p.add_argument("--warmup", type=int, default=500)
    p.add_argument("--weight_decay", type=float, default=0.01)
    p.add_argument("--max_length", type=int, default=512)
    p.add_argument("--save_strategy", default="epoch")
    p.add_argument("--eval_strategy", default="epoch")
    p.add_argument("--logging_steps", type=int, default=100)
    p.add_argument("--seed", type=int, default=42)
    p.add_argument("--push", action="store_true", help="Push model to HF Hub")
    p.add_argument(
        "--hub_id", type=str, default=None, help="HF repo id (org/model)"
    )
    
    # üéØ NOUVEAU argument pour la colonne cible
    p.add_argument("--target-column", choices=["label", "importance"], default="label",
                   help="Colonne √† utiliser pour l'entra√Ænement (label=sentiment, importance=importance)")
    
    # üöÄ NOUVEAUX arguments pour l'apprentissage incr√©mental
    p.add_argument("--incremental", action="store_true", 
                   help="Activer l'apprentissage incr√©mental")
    p.add_argument("--mode", choices=["test", "development", "production"], default="test",
                   help="Mode incr√©mental (test/development/production)")
    p.add_argument("--baseline-model", type=str, default=None,
                   help="Mod√®le de r√©f√©rence pour l'incr√©mental (auto selon mode si non sp√©cifi√©)")
    p.add_argument("--min-improvement", type=float, default=0.02,
                   help="Am√©lioration minimum requise pour mise √† jour (d√©faut: 0.02)")
    p.add_argument("--force-update", action="store_true",
                   help="Forcer la mise √† jour m√™me sans am√©lioration significative")
    
    return p


# ---------------------------------------------------------------------------
# Entr√©e principale (adapt√©e pour supporter l'incr√©mental + importance)
# ---------------------------------------------------------------------------
def main():
    args = build_parser().parse_args()
    set_seed(args.seed)

    # Auto-s√©lection dataset (existant, conserv√©)
    if args.dataset is None and AUTOSEL:
        args.dataset = latest_dataset()
        if args.dataset:
            logger.info("üïµÔ∏è  Auto-s√©lection dataset : %s", args.dataset)
        else:
            logger.error("‚ùå Aucun dataset trouv√©")
            logger.info(
                "üí° Ajoutez des fichiers au format news_YYYYMMDD.csv "
                "dans datasets/"
            )
            return
    elif args.dataset is None:
        logger.error(
            "‚ùå Aucun dataset sp√©cifi√© et auto-s√©lection non disponible"
        )
        logger.info(
            "üí° Utilisez: python scripts/finetune.py "
            "--dataset datasets/votre_fichier.csv --output_dir models/test"
        )
        return

    # üöÄ NOUVEAU : Gestion du mode incr√©mental
    if args.incremental:
        logger.info("üîÑ Mode apprentissage incr√©mental activ√©")
        
        # D√©terminer le mod√®le de base selon le mode
        if getattr(args, 'baseline_model', None) is None:
            args.baseline_model = MODELS_CONFIG.get(args.mode, {}).get("hf_id", "yiyanghkust/finbert-tone")
        
        logger.info(f"üéØ Mode: {args.mode}")
        logger.info(f"üì¶ Mod√®le de base: {args.baseline_model}")
        
        # Initialiser le fine-tuner en mode incr√©mental
        tuner = Finetuner(
            model_name=args.model_name, 
            max_length=args.max_length,
            incremental_mode=True,
            baseline_model=args.baseline_model,
            target_column=args.target_column  # üéØ NOUVEAU
        )
        
        # Chargement du dataset avec division train/val/test
        ds, test_ds = tuner.load_dataset(args.dataset)
        
        # √âvaluation baseline du mod√®le existant (seulement si test_ds existe et non vide)
        baseline_metrics = None
        if test_ds is not None and len(test_ds) > 0:
            logger.info("üìä √âvaluation baseline du mod√®le existant...")
            baseline_metrics = tuner.evaluate_on_test(test_ds)
            logger.info(f"üìä M√©triques baseline: {baseline_metrics}")
        else:
            logger.warning("‚ö†Ô∏è Pas de dataset de test pour √©valuation baseline")
            # M√©triques factices pour le test
            baseline_metrics = {"accuracy": 0.5, "f1": 0.5, "precision": 0.5, "recall": 0.5}
        
        # Entra√Ænement incr√©mental
        test_metrics = tuner.train(ds, args, test_ds)
        
        # D√©cision de mise √† jour
        if test_metrics and baseline_metrics:
            logger.info("üîç Analyse des r√©sultats pour apprentissage incr√©mental...")
            
            should_update, reason = tuner.should_update_model(
                baseline_metrics, 
                test_metrics,
                getattr(args, 'min_improvement', 0.02)
            )
            
            logger.info(f"üìä D√©cision de mise √† jour: {reason}")
            
            # Forcer la mise √† jour si demand√©
            if getattr(args, 'force_update', False) and not should_update:
                should_update = True
                reason = f"Mise √† jour forc√©e (--force-update). {reason}"
                logger.warning(f"‚ö†Ô∏è {reason}")
            
            if should_update:
                # Mise √† jour du mod√®le sur HuggingFace
                if args.mode in ["production", "development"]:
                    hf_model_id = MODELS_CONFIG[args.mode]["hf_id"]
                    commit_msg = f"Apprentissage incr√©mental - Accuracy: {test_metrics['accuracy']:.3f}, F1: {test_metrics['f1']:.3f}"
                    
                    logger.info(f"üöÄ Mise √† jour du mod√®le {args.mode}: {hf_model_id}")
                    tuner.push_to_huggingface(args.output_dir, hf_model_id, commit_msg)
                    
                    logger.info("‚úÖ Mod√®le de production mis √† jour!")
                else:
                    logger.info("‚úÖ Nouveau mod√®le valid√© (mode test - pas de mise √† jour auto)")
            else:
                logger.info(f"‚è∏Ô∏è Mod√®le non mis √† jour: {reason}")
            
            # Mise √† jour du rapport avec la d√©cision
            report_path = args.output_dir / "incremental_training_report.json"
            if report_path.exists():
                with open(report_path, "r") as f:
                    report = json.load(f)
                
                report.update({
                    "baseline_metrics": baseline_metrics,
                    "new_metrics": test_metrics,
                    "model_updated": should_update,
                    "update_reason": reason,
                    "hf_model_id": MODELS_CONFIG.get(args.mode, {}).get("hf_id") if should_update else None,
                })
                
                with open(report_path, "w") as f:
                    json.dump(report, f, indent=2)
        else:
            # Pas d'√©valuation possible, forcer la mise √† jour si demand√©
            logger.warning("‚ö†Ô∏è Pas d'√©valuation possible, mise √† jour conditionnelle")
            should_update = getattr(args, 'force_update', False)
            
            if should_update and args.mode in ["production", "development"]:
                hf_model_id = MODELS_CONFIG[args.mode]["hf_id"]
                commit_msg = f"Apprentissage incr√©mental - Dataset petit"
                
                logger.info(f"üöÄ Mise √† jour forc√©e du mod√®le {args.mode}: {hf_model_id}")
                tuner.push_to_huggingface(args.output_dir, hf_model_id, commit_msg)
                
                # Mise √† jour du rapport
                report_path = args.output_dir / "incremental_training_report.json"
                if report_path.exists():
                    with open(report_path, "r") as f:
                        report = json.load(f)
                    
                    report.update({
                        "baseline_metrics": baseline_metrics,
                        "new_metrics": test_metrics or {"accuracy": 0.0, "f1": 0.0},
                        "model_updated": should_update,
                        "update_reason": "Force update - petit dataset",
                        "hf_model_id": hf_model_id,
                    })
                    
                    with open(report_path, "w") as f:
                        json.dump(report, f, indent=2)
        
    else:
        # üîÑ Mode classique (existant, conserv√© exactement)
        logger.info("üîÑ Mode classique activ√©")
        
        # Auto-g√©n√©ration nom de mod√®le bas√© sur la date du dataset (existant, conserv√©)
        if AUTOSEL and args.dataset:
            date_str = get_date_from_filename(str(args.dataset))
            if date_str and str(args.output_dir).endswith("-auto"):
                # Si output_dir se termine par -auto, on le remplace par la date
                new_output = str(args.output_dir).replace("-auto", f"-{date_str}")
                args.output_dir = Path(new_output)
                logger.info("üìÇ Nom de mod√®le auto-g√©n√©r√© : %s", args.output_dir)

        # Support du mod√®le baseline pour l'incr√©mental (AJOUT√â)
        baseline_model = getattr(args, 'baseline_model', None) if hasattr(args, 'baseline_model') else None
        
        if baseline_model and args.incremental:
            tuner = Finetuner(
                model_name=args.model_name, 
                max_length=args.max_length,
                incremental_mode=True,
                baseline_model=baseline_model,
                target_column=args.target_column  # üéØ NOUVEAU
            )
        else:
            tuner = Finetuner(
                model_name=args.model_name, 
                max_length=args.max_length,
                target_column=args.target_column  # üéØ NOUVEAU
            )

        ds, _ = tuner.load_dataset(args.dataset)
        tuner.train(ds, args)

        # Push classique si demand√© (existant, conserv√©)
        if args.push:
            logger.info("üì§ Push du mod√®le vers HuggingFace Hub...")


if __name__ == "__main__":
    main()
