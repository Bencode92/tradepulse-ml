#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TradePulse News Collector - Smart Triple-Model ML Labeling
==========================================================

Collecte avec triple labellisation automatique:
- Sentiment: positive/negative/neutral
- Importance: critique/importante/g√©n√©rale
- Correlations: d√©tection ML des impacts sur commodit√©s

Usage:
    python scripts/collect_news.py --source fmp --count 60 --days 7 --auto-label
"""

import argparse
import csv
import datetime
import json
import logging
import os
import random
import zoneinfo
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
import requests
import time
import sys
import numpy as np
from collections import defaultdict

# Configuration des logs
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s"
)
logger = logging.getLogger("smart-collector")

# Fuseau horaire Paris
PARIS_TZ = zoneinfo.ZoneInfo("Europe/Paris")

# üîß FIX: Ajouter le chemin parent pour les imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import CommodityCorrelator
try:
    # Essayer d'importer depuis stock-analysis-platform
    platform_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 
                                 "stock-analysis-platform", "scripts")
    if os.path.exists(platform_path):
        sys.path.append(platform_path)
        from commodity_correlator import CommodityCorrelator
        CORRELATOR_AVAILABLE = True
        logger.info("‚úÖ CommodityCorrelator import√© avec succ√®s")
    else:
        CORRELATOR_AVAILABLE = False
        logger.warning("‚ö†Ô∏è CommodityCorrelator non disponible - chemin non trouv√©")
except ImportError as e:
    CORRELATOR_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è CommodityCorrelator non disponible: {e}")

# üéØ MOD√àLES SP√âCIALIS√âS PRODUITS PAR LE WORKFLOW
ML_MODELS_CONFIG = {
    "sentiment": "Bencode92/tradepulse-finbert-sentiment",    # ‚úÖ Mod√®le sentiment entra√Æn√©
    "importance": "Bencode92/tradepulse-finbert-importance",  # ‚úÖ Mod√®le importance entra√Æn√©
    "correlation": "Bencode92/tradepulse-finbert-correlations",  # ‚úÖ Mod√®le corr√©lation ML
    "production": "Bencode92/tradepulse-finbert-sentiment",   # Alias pour compatibilit√©
    "fallback": "yiyanghkust/finbert-tone",                   # Fallback de base
}

# Configuration importance (reprend de fmp_news_updater.py)
KEYWORD_TIERS = {
    "high": [
        # Chocs march√© & macro
        "crash", "collapse", "contagion", "default", "downgrade", "stagflation",
        "recession", "sovereign risk", "yield spike", "volatility spike",
        # Banques centrales / inflation
        "cpi", "pce", "core inflation", "rate hike", "rate cut", "qt", "qe",
        # Cr√©dit & liquidit√©
        "credit spread", "cds", "insolvency", "liquidity crunch",
        # Fondamentaux entreprise
        "profit warning", "guidance cut", "eps miss", "dividend cut",
        # G√©opolitique
        "sanction", "embargo", "war", "conflict"
    ],
    "medium": [
        "earnings beat", "eps beat", "revenue beat", "free cash flow",
        "buyback", "merger", "acquisition", "spin-off", "ipo", "stake sale",
        "job cuts", "strike", "production halt", "regulation", "antitrust",
        "fine", "class action", "data breach", "rating watch",
        "payrolls", "unemployment rate", "pmi", "ism", "consumer confidence",
        "ppi", "housing starts"
    ]
}

PREMIUM_SOURCES = ["bloomberg", "reuters", "financial times", "wall street journal"]

# Configuration FMP
FMP_ENDPOINTS = {
    "general_news": "https://financialmodelingprep.com/api/v3/fmp/articles",
    "stock_news": "https://financialmodelingprep.com/api/v3/stock_news",
    "crypto_news": "https://financialmodelingprep.com/api/v4/crypto_news",
    "forex_news": "https://financialmodelingprep.com/api/v4/forex_news",
    "press_releases": "https://financialmodelingprep.com/api/v3/press-releases"
}

FMP_LIMITS = {
    "general_news": 20,
    "stock_news": 50,
    "crypto_news": 15,
    "forex_news": 10,
    "press_releases": 5
}

class SmartNewsCollector:
    """Collecteur avec triple ML (sentiment + importance + correlations) - MOD√àLES PRODUITS"""
    
    def __init__(self, output_dir: str = "datasets", enable_cache: bool = True, 
                 auto_label: bool = False, ml_model: str = "production",
                 confidence_threshold: float = 0.75):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.enable_cache = enable_cache
        self.cache_file = self.output_dir / ".article_cache.json"
        self.seen_articles: Set[str] = set()
        
        # API Key
        self.api_key = os.getenv("FMP_API_KEY")
        if not self.api_key:
            raise ValueError("FMP_API_KEY environment variable required")
        
        # ML Configuration
        self.auto_label = auto_label
        self.ml_model = ml_model
        self.confidence_threshold = confidence_threshold
        self.sentiment_classifier = None
        self.importance_classifier = None
        self.correlation_classifier = None  # Nouveau mod√®le ML
        self.commodity_codes = []  # üîß FIX: Initialiser vide
        
        # Compteur de labels pour ajustement par raret√©
        self.label_counts = defaultdict(int)
        
        # Commodity Correlator (fallback)
        self.correlator = CommodityCorrelator() if CORRELATOR_AVAILABLE else None
        if self.correlator:
            logger.info("üîó D√©tection des corr√©lations commodit√©s activ√©e (r√®gles)")
        
        self._load_cache()
        
        if self.auto_label:
            self._load_ml_models()

    def _load_cache(self):
        """Charge le cache de d√©duplication"""
        if not self.enable_cache or not self.cache_file.exists():
            return
        
        try:
            with open(self.cache_file, "r", encoding="utf-8") as f:
                cache_data = json.load(f)
                self.seen_articles = set(cache_data.get("articles", []))
            logger.info(f"üóÑÔ∏è Cache charg√©: {len(self.seen_articles)} articles connus")
        except Exception as e:
            logger.warning(f"Erreur chargement cache: {e}")
            self.seen_articles = set()

    def _save_cache(self):
        """Sauvegarde le cache de d√©duplication"""
        if not self.enable_cache:
            return
            
        try:
            cache_data = {"articles": list(self.seen_articles)}
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, indent=2)
        except Exception as e:
            logger.warning(f"Erreur sauvegarde cache: {e}")

    def _load_ml_models(self):
        """üéØ Charge les 3 mod√®les sp√©cialis√©s PRODUITS par le workflow"""
        try:
            from transformers import pipeline
            import torch
            
            hf_token = os.getenv("HF_TOKEN")
            model_kwargs = {"token": hf_token} if hf_token else {}
            device = 0 if torch.cuda.is_available() else -1
            
            # 1. Mod√®le sentiment PRODUIT
            try:
                logger.info(f"üòä Chargement mod√®le sentiment PRODUIT: {ML_MODELS_CONFIG['sentiment']}")
                self.sentiment_classifier = pipeline(
                    "text-classification",
                    model=ML_MODELS_CONFIG["sentiment"],
                    return_all_scores=True,
                    device=device,
                    **model_kwargs
                )
                logger.info("‚úÖ Mod√®le sentiment PRODUIT charg√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Fallback sentiment: {e}")
                self.sentiment_classifier = pipeline(
                    "text-classification",
                    model=ML_MODELS_CONFIG["fallback"],
                    return_all_scores=True,
                    device=device
                )
            
            # 2. Mod√®le importance PRODUIT
            try:
                logger.info(f"üéØ Chargement mod√®le importance PRODUIT: {ML_MODELS_CONFIG['importance']}")
                self.importance_classifier = pipeline(
                    "text-classification",
                    model=ML_MODELS_CONFIG["importance"],
                    return_all_scores=True,
                    device=device,
                    **model_kwargs
                )
                logger.info("‚úÖ Mod√®le importance PRODUIT charg√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Pas de mod√®le importance, utilisation r√®gles: {e}")
                self.importance_classifier = None
            
            # 3. Mod√®le corr√©lation ML (NOUVEAU)
            try:
                logger.info(f"üîó Chargement mod√®le corr√©lation ML: {ML_MODELS_CONFIG['correlation']}")

                # Charger le mapping local si dispo
                try:
                    from config.correlation_mapping import COMMODITY_CODES
                    self.commodity_codes = COMMODITY_CODES
                    logger.info(f"üìä {len(COMMODITY_CODES)} codes de commodit√©s charg√©s (local)")
                except ImportError:
                    logger.warning("‚ö†Ô∏è correlation_mapping.py non trouv√© ‚Äî tentative lecture id2label HF")
                    self.commodity_codes = []

                # Charger le pipeline (sigmo√Øde + top_k=None)
                from transformers import pipeline, AutoConfig
                hf_token = os.getenv("HF_TOKEN")
                model_kwargs = {"token": hf_token} if hf_token else {}
                self.correlation_classifier = pipeline(
                    "text-classification",
                    model=ML_MODELS_CONFIG["correlation"],
                    device=device,
                    top_k=None,                    # renvoyer tous les scores
                    function_to_apply="sigmoid",   # multi-label
                    truncation=True,
                    **model_kwargs
                )

                # Si pas de mapping local, lire id2label du config HF
                if not self.commodity_codes:
                    try:
                        cfg = AutoConfig.from_pretrained(ML_MODELS_CONFIG["correlation"], **model_kwargs)
                        if hasattr(cfg, "id2label") and isinstance(cfg.id2label, dict):
                            # ordonner par indice
                            ids = sorted((int(k), v) for k, v in cfg.id2label.items())
                            self.commodity_codes = [v for _, v in ids]
                            logger.info(f"üìë id2label HF: {len(self.commodity_codes)} labels charg√©s")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Impossible de lire id2label HF: {e}")

                logger.info("‚úÖ Mod√®le corr√©lation ML charg√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Pas de mod√®le corr√©lation ML: {e}")
                self.correlation_classifier = None
                
        except Exception as e:
            logger.error(f"‚ùå Impossible de charger les mod√®les: {e}")
            self.sentiment_classifier = None
            self.importance_classifier = None
            self.correlation_classifier = None

    def _adaptive_threshold(self, arr, p=0.97, t_min=0.22, t_max=0.55):
        """Seuil par percentile, born√©."""
        if len(arr) == 0:
            return t_max
        q = float(np.quantile(arr, p))
        return float(min(t_max, max(t_min, q)))

    def _predict_correlations_ml(self, text: str) -> List[str]:
        """üîó Pr√©dit les corr√©lations avec seuil adaptatif bas√© sur la distribution des scores."""
        if not self.correlation_classifier:
            return []

        try:
            preds = self.correlation_classifier(text, top_k=None)
            # le pipeline peut renvoyer [[{...}]] ou [{...}]
            if isinstance(preds, list) and preds and isinstance(preds[0], list):
                preds = preds[0]

            scores = [(p.get("label", ""), float(p.get("score", 0.0))) for p in preds]
            scores.sort(key=lambda x: x[1], reverse=True)

            # seuil adaptatif par distribution de l'ARTICLE
            arr = [sc for _, sc in scores]
            T = self._adaptive_threshold(arr, p=0.97, t_min=0.22, t_max=0.55)

            # ajustement l√©ger selon raret√© (optionnel)
            # rares -> -0.02, fr√©quents -> +0.02
            def rarity_shift(label: str) -> float:
                c = self.label_counts.get(label, 0)
                if c >= 10:    # tr√®s fr√©quent dans la session
                    return +0.02
                if c <= 1:     # rare
                    return -0.02
                return 0.0

            picked = []
            for lab, sc in scores:
                thr = T + rarity_shift(lab)
                if sc >= thr:
                    picked.append(lab)

            # Fallback TOP1 si rien mais signal net
            if not picked and scores:
                top_lab, top_sc = scores[0]
                if top_sc >= 0.40:
                    picked = [top_lab]

            # limiter le bruit
            MAX_LABELS = 3
            picked = picked[:MAX_LABELS]

            # liste blanche √©ventuelle
            if self.commodity_codes:
                picked = [lab for lab in picked if lab in self.commodity_codes]

            # maj compteurs
            for lab in picked:
                self.label_counts[lab] += 1

            return picked

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur pr√©diction corr√©lations ML: {e}")
            return []

    def _detect_correlations(self, text: str) -> List[str]:
        """D√©tecte les corr√©lations - ML d'abord, puis fallback CommodityCorrelator"""
        correlations = []
        
        # 1. Essayer d'abord avec le mod√®le ML
        if self.correlation_classifier:
            correlations = self._predict_correlations_ml(text)
            if correlations:
                return correlations
        
        # 2. Fallback sur CommodityCorrelator si disponible
        if not correlations and self.correlator:
            try:
                # Utiliser les m√™mes filtres que commodity_correlator
                if self.correlator._is_company_article(text):
                    return []
                if not self.correlator._is_macro_article(text):
                    return []
                
                # D√©tecter les pays mentionn√©s
                detected_countries = self.correlator.detect_countries_from_text(text)
                
                for country in detected_countries:
                    # Obtenir les exports du pays
                    country_exports = self.correlator.get_country_exports(country)
                    
                    for export in country_exports:
                        # Filtrer comme dans commodity_correlator
                        if export.get("impact") not in ("pivot", "major"):
                            continue
                        
                        # V√©rifier si le produit est mentionn√©
                        if self.correlator._mentions_product(text, export["product_code"]):
                            correlations.append(f"{country}:{export['product_code']}")
                
            except Exception as e:
                logger.debug(f"Erreur d√©tection corr√©lations r√®gles: {e}")
        
        return correlations

    def _predict_triple_labels(self, text: str) -> Tuple[str, str, List[str], float, float]:
        """üéØ Pr√©dit sentiment, importance ET corr√©lations avec les mod√®les PRODUITS"""
        text_truncated = text[:512]
        
        # 1. Pr√©diction sentiment avec mod√®le PRODUIT
        sentiment_label = "neutral"
        sentiment_confidence = 0.5
        
        if self.sentiment_classifier:
            try:
                results = self.sentiment_classifier(text_truncated)
                best_pred = max(results[0], key=lambda x: x['score'])
                
                # Normalisation labels sentiment (sp√©cialis√© PRODUIT)
                label_mapping = {
                    'POSITIVE': 'positive', 'NEGATIVE': 'negative', 'NEUTRAL': 'neutral',
                    'positive': 'positive', 'negative': 'negative', 'neutral': 'neutral',
                    'LABEL_0': 'negative', 'LABEL_1': 'neutral', 'LABEL_2': 'positive',
                }
                
                sentiment_label = label_mapping.get(best_pred['label'], 'neutral')
                sentiment_confidence = best_pred['score']
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur pr√©diction sentiment: {e}")
                sentiment_label = self._basic_sentiment_analysis(text)
        else:
            sentiment_label = self._basic_sentiment_analysis(text)
        
        # 2. Pr√©diction importance avec mod√®le PRODUIT
        importance_label = "g√©n√©rale"
        importance_confidence = 0.5
        
        if self.importance_classifier:
            try:
                results = self.importance_classifier(text_truncated)
                best_pred = max(results[0], key=lambda x: x['score'])
                
                # Normalisation labels importance (sp√©cialis√© PRODUIT)
                importance_mapping = {
                    'critique': 'critique', 'importante': 'importante', 'g√©n√©rale': 'g√©n√©rale',
                    'LABEL_0': 'g√©n√©rale', 'LABEL_1': 'importante', 'LABEL_2': 'critique',
                }
                
                importance_label = importance_mapping.get(best_pred['label'], 'g√©n√©rale')
                importance_confidence = best_pred['score']
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur pr√©diction importance: {e}")
                importance_label = self._basic_importance_analysis(text)
        else:
            importance_label = self._basic_importance_analysis(text)
        
        # 3. Pr√©diction corr√©lations (ML ou r√®gles)
        correlations = self._detect_correlations(text)
        
        return sentiment_label, importance_label, correlations, sentiment_confidence, importance_confidence

    def _basic_sentiment_analysis(self, text: str) -> str:
        """Analyse de sentiment basique (fallback)"""
        text_lower = text.lower()
        
        positive_words = ["gain", "rise", "surge", "rally", "beat", "growth", "strong", "bullish"]
        negative_words = ["drop", "fall", "decline", "crash", "loss", "weak", "bearish", "concern"]
        
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        if pos_count > neg_count:
            return "positive"
        elif neg_count > pos_count:
            return "negative"
        else:
            return "neutral"

    def _basic_importance_analysis(self, text: str) -> str:
        """üéØ Analyse d'importance basique avec mots-cl√©s (fallback)"""
        text_lower = text.lower()
        
        high_score = sum(1 for kw in KEYWORD_TIERS["high"] if kw in text_lower)
        medium_score = sum(1 for kw in KEYWORD_TIERS["medium"] if kw in text_lower)
        
        if high_score >= 2:
            return "critique"
        elif high_score >= 1 or medium_score >= 3:
            return "importante"
        else:
            return "g√©n√©rale"

    def _article_hash(self, text: str) -> str:
        """G√©n√®re un hash unique pour un article"""
        import hashlib
        normalized = text.lower().strip()
        normalized = "".join(c for c in normalized if c.isalnum() or c.isspace())
        return hashlib.md5(normalized.encode()).hexdigest()

    def _is_duplicate(self, text: str) -> bool:
        """V√©rifie si un article est un doublon"""
        if not self.enable_cache:
            return False
        return self._article_hash(text) in self.seen_articles

    def _add_to_cache(self, text: str):
        """Ajoute un article au cache"""
        if self.enable_cache:
            self.seen_articles.add(self._article_hash(text))

    def fetch_fmp_data(self, endpoint: str, params: Dict = None) -> List[Dict]:
        """R√©cup√®re des donn√©es depuis l'API FMP"""
        if params is None:
            params = {}
        
        params["apikey"] = self.api_key
        
        try:
            logger.info(f"üì° Requ√™te FMP: {endpoint}")
            response = requests.get(endpoint, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if isinstance(data, list):
                logger.info(f"‚úÖ {len(data)} articles r√©cup√©r√©s de FMP")
                return data
            else:
                logger.warning(f"‚ö†Ô∏è Format de r√©ponse inattendu: {type(data)}")
                return []
                
        except Exception as e:
            logger.error(f"‚ùå Erreur API FMP {endpoint}: {e}")
            return []

    def fetch_articles_by_period(self, endpoint: str, start_date: str, end_date: str, 
                                limit: int = 50, days_interval: int = 7) -> List[Dict]:
        """R√©cup√®re des articles sur une p√©riode donn√©e avec pagination"""
        from datetime import datetime, timedelta
        
        logger.info(f"üìÖ Collecte FMP: {start_date} √† {end_date} (limite: {limit})")
        
        from_date = datetime.strptime(start_date, "%Y-%m-%d")
        to_date = datetime.strptime(end_date, "%Y-%m-%d")
        all_articles = []
        
        # Traitement par intervalles
        current_from = from_date
        while current_from < to_date and len(all_articles) < limit:
            current_to = min(current_from + timedelta(days=days_interval), to_date)
            
            # Pagination par intervalle
            for page in range(3):  # Max 3 pages par intervalle
                params = {
                    "from": current_from.strftime("%Y-%m-%d"),
                    "to": current_to.strftime("%Y-%m-%d"),
                    "page": page,
                    "limit": min(50, limit - len(all_articles))
                }
                
                articles = self.fetch_fmp_data(endpoint, params)
                
                if not articles:
                    break
                
                # Filtrage et ajout
                for article in articles:
                    title = article.get("title", "")
                    content = article.get("text", "") or article.get("content", "")
                    
                    if len(title) >= 20 and len(content) >= 100:
                        if not self._is_duplicate(title):
                            all_articles.append(article)
                            self._add_to_cache(title)
                            
                            if len(all_articles) >= limit:
                                break
                
                if len(articles) < params["limit"] or len(all_articles) >= limit:
                    break
            
            current_from = current_to
        
        logger.info(f"üìä Articles collect√©s: {len(all_articles)}")
        return all_articles

    def collect_fmp_news(self, count: int = 40, days: int = 7) -> List[Dict]:
        """Collecte des actualit√©s depuis FMP avec r√©partition intelligente"""
        today = datetime.datetime.now(PARIS_TZ).date()
        start_date = (today - datetime.timedelta(days=days)).strftime("%Y-%m-%d")
        end_date = today.strftime("%Y-%m-%d")
        
        all_articles = []
        
        # R√©partition par endpoint
        for endpoint_name, endpoint_url in FMP_ENDPOINTS.items():
            limit = FMP_LIMITS.get(endpoint_name, 10)
            # Ajuster selon le count total demand√©
            adjusted_limit = int(limit * (count / 100))  # Proportion du total
            
            if adjusted_limit < 1:
                continue
                
            logger.info(f"üîç Collecte {endpoint_name}: {adjusted_limit} articles max")
            
            articles = self.fetch_articles_by_period(
                endpoint_url, start_date, end_date, adjusted_limit
            )
            
            # Normalisation et enrichissement
            for article in articles:
                enriched = self._enrich_article(article, endpoint_name)
                if enriched:
                    all_articles.append(enriched)
        
        # Tri par qualit√©
        all_articles.sort(key=lambda x: x.get("quality_score", 0), reverse=True)
        
        return all_articles[:count]

    def _enrich_article(self, article: Dict, source_type: str) -> Optional[Dict]:
        """üéØ Enrichit un article avec triple labellisation + correlations"""
        try:
            title = article.get("title", "")
            content = article.get("text", "") or article.get("content", "")
            
            if not title or not content:
                return None
            
            # Structure normalis√©e
            enriched = {
                "text": f"{title}. {content}".strip(),
                "title": title,
                "summary": content[:200] + "..." if len(content) > 200 else content,
                "source": article.get("site", "") or article.get("publisher", "FMP"),
                "url": article.get("url", ""),
                "published_date": article.get("publishedDate", ""),
                "collected_at": datetime.datetime.now(PARIS_TZ).isoformat(),
                "source_type": source_type
            }
            
            # üéØ Triple pr√©diction avec MOD√àLES PRODUITS
            if self.auto_label:
                sentiment_label, importance_label, correlations, sent_conf, imp_conf = self._predict_triple_labels(enriched["text"])
                
                enriched.update({
                    "label": sentiment_label,
                    "importance": importance_label,
                    "correlations": correlations,
                    "sentiment_confidence": sent_conf,
                    "importance_confidence": imp_conf,
                    "needs_review": sent_conf < self.confidence_threshold or imp_conf < self.confidence_threshold,
                    "sentiment_model": ML_MODELS_CONFIG["sentiment"] if self.sentiment_classifier else "rule_based",
                    "importance_model": ML_MODELS_CONFIG["importance"] if self.importance_classifier else "rule_based",
                    "correlation_model": ML_MODELS_CONFIG["correlation"] if self.correlation_classifier else "commodity_correlator",
                    "labeling_method": "triple_ml_produit"
                })
            else:
                sentiment_label = self._basic_sentiment_analysis(enriched["text"])
                importance_label = self._basic_importance_analysis(enriched["text"])
                correlations = self._detect_correlations(enriched["text"])
                
                enriched.update({
                    "label": sentiment_label,
                    "importance": importance_label,
                    "correlations": correlations,
                    "sentiment_confidence": None,
                    "importance_confidence": None,
                    "needs_review": False,
                    "labeling_method": "rule_based_triple"
                })
            
            # Score qualit√© global
            enriched["quality_score"] = self._calculate_quality_score(enriched)
            
            return enriched
            
        except Exception as e:
            logger.warning(f"Erreur enrichissement article: {e}")
            return None

    def _calculate_quality_score(self, article: Dict) -> float:
        """Calcule un score de qualit√©"""
        # Base: longueur
        title_len = len(article.get("title", ""))
        text_len = len(article.get("text", ""))
        
        score = min(20, title_len / 5) + min(30, text_len / 100)
        
        # Bonus confiance ML des MOD√àLES PRODUITS
        if article.get("sentiment_confidence"):
            score += article["sentiment_confidence"] * 10
        if article.get("importance_confidence"):
            score += article["importance_confidence"] * 10
        
        # Bonus source premium
        source = article.get("source", "").lower()
        if any(premium in source for premium in PREMIUM_SOURCES):
            score += 25
        
        # Bonus mots-cl√©s importants
        text_lower = article.get("text", "").lower()
        high_kw = sum(1 for kw in KEYWORD_TIERS["high"] if kw in text_lower)
        score += high_kw * 5
        
        # üîó NOUVEAU: Bonus pour corr√©lations d√©tect√©es
        correlations = article.get("correlations", [])
        score += len(correlations) * 3
        
        return min(100, score)

    def save_dataset(self, articles: List[Dict], output_file: Optional[Path] = None) -> Path:
        """üéØ Sauvegarde avec 4 colonnes: text, label, importance, correlations"""
        if output_file is None:
            today = datetime.datetime.now(PARIS_TZ).strftime("%Y%m%d")
            output_file = self.output_dir / f"news_{today}.csv"

        # Sauvegarde CSV avec 4 colonnes
        with open(output_file, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["text", "label", "importance", "correlations"])  # 4 colonnes
            
            for article in articles:
                # Joindre les corr√©lations avec des virgules
                correlations_str = ",".join(article.get("correlations", []))
                
                writer.writerow([
                    article["text"], 
                    article["label"], 
                    article["importance"],
                    correlations_str
                ])

        # M√©tadonn√©es JSON enrichies
        labels = [article["label"] for article in articles]
        importance_labels = [article["importance"] for article in articles]
        
        label_counts = {label: labels.count(label) for label in set(labels)}
        importance_counts = {label: importance_labels.count(label) for label in set(importance_labels)}
        
        # Statistiques des corr√©lations
        all_correlations = []
        for article in articles:
            all_correlations.extend(article.get("correlations", []))
        
        correlation_counts = {}
        for corr in all_correlations:
            correlation_counts[corr] = correlation_counts.get(corr, 0) + 1
        
        needs_review_count = sum(1 for article in articles if article.get("needs_review", False))
        
        metadata = {
            "filename": output_file.name,
            "created_at": datetime.datetime.now(PARIS_TZ).isoformat(),
            "source": "fmp_smart_triple_ml",
            "article_count": len(articles),
            "label_distribution": label_counts,
            "importance_distribution": importance_counts,
            "correlation_distribution": dict(sorted(correlation_counts.items(), key=lambda x: x[1], reverse=True)[:20]),
            "total_correlations": len(all_correlations),
            "unique_correlations": len(set(all_correlations)),
            "correlator_enabled": CORRELATOR_AVAILABLE,
            "ml_correlation_enabled": self.correlation_classifier is not None,
            "deduplication_enabled": self.enable_cache,
            "cache_size": len(self.seen_articles),
            "triple_ml_enabled": self.auto_label,
            "sentiment_model": ML_MODELS_CONFIG["sentiment"] if self.auto_label else None,
            "importance_model": ML_MODELS_CONFIG["importance"] if self.auto_label else None,
            "correlation_model": ML_MODELS_CONFIG["correlation"] if self.correlation_classifier else None,
            "confidence_threshold": self.confidence_threshold if self.auto_label else None,
            "high_confidence_articles": len(articles) - needs_review_count,
            "needs_review_articles": needs_review_count,
            "models_source": "workflow_produit_triple"
        }

        json_file = output_file.with_suffix('.json')
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        # Sauvegarde cache
        self._save_cache()

        logger.info(f"‚úÖ Dataset Smart PRODUIT: {output_file} ({len(articles)} √©chantillons)")
        logger.info(f"üîó Corr√©lations d√©tect√©es: {len(all_correlations)} total, {len(set(all_correlations))} uniques")
        return output_file

    def collect_and_save(self, count: int = 40, days: int = 7, output_file: Optional[Path] = None) -> Path:
        """üéØ Pipeline complet avec MOD√àLES PRODUITS + corr√©lations"""
        logger.info(f"üöÄ Collecte Smart PRODUIT: {count} articles, {days} jours")
        
        if self.auto_label:
            logger.info(f"üéØ Triple ML PRODUIT activ√©: sentiment + importance + corr√©lations")
            logger.info(f"üòä Mod√®le sentiment: {ML_MODELS_CONFIG['sentiment']}")
            logger.info(f"üéØ Mod√®le importance: {ML_MODELS_CONFIG['importance']}")
            if self.correlation_classifier:
                logger.info(f"üîó Mod√®le corr√©lation ML: {ML_MODELS_CONFIG['correlation']}")
        
        if CORRELATOR_AVAILABLE and not self.correlation_classifier:
            logger.info(f"üîó D√©tection des corr√©lations commodit√©s par r√®gles (fallback)")

        # Collecte
        articles = self.collect_fmp_news(count, days)
        
        if not articles:
            raise RuntimeError("Aucun article FMP collect√©")

        # Statistiques
        labels = [article["label"] for article in articles]
        importance_labels = [article["importance"] for article in articles]
        
        label_counts = {label: labels.count(label) for label in set(labels)}
        importance_counts = {label: importance_labels.count(label) for label in set(importance_labels)}
        
        # Statistiques corr√©lations
        correlation_stats = {}
        articles_with_corr = 0
        for article in articles:
            corr_list = article.get("correlations", [])
            if corr_list:
                articles_with_corr += 1
            for corr in corr_list:
                correlation_stats[corr] = correlation_stats.get(corr, 0) + 1
        
        logger.info(f"üìä Distribution sentiment: {label_counts}")
        logger.info(f"üéØ Distribution importance: {importance_counts}")
        if correlation_stats:
            top_correlations = sorted(correlation_stats.items(), key=lambda x: x[1], reverse=True)[:5]
            logger.info(f"üîó Top 5 corr√©lations: {top_correlations}")
        
        # Statistiques de session
        if self.label_counts:
            top_labels = sorted(self.label_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            logger.info(f"üîó Top labels corr√©lation (session): {top_labels}")
        
        logger.info(f"üìà Articles avec ‚â•1 corr√©lation: {articles_with_corr}/{len(articles)} ({100*articles_with_corr/len(articles):.1f}%)")
        logger.info(f"üóÑÔ∏è Cache: {len(self.seen_articles)} articles connus")
        
        if self.auto_label:
            needs_review_count = sum(1 for article in articles if article.get("needs_review", False))
            high_confidence_count = len(articles) - needs_review_count
            logger.info(f"üéØ Articles haute confiance PRODUIT: {high_confidence_count}/{len(articles)}")

        return self.save_dataset(articles, output_file)


def main():
    parser = argparse.ArgumentParser(description="Smart News Collector with Triple ML")
    
    parser.add_argument("--source", choices=["fmp"], default="fmp", help="Source FMP")
    parser.add_argument("--count", type=int, default=40, help="Nombre d'articles")
    parser.add_argument("--days", type=int, default=7, help="Fen√™tre temporelle en jours")
    parser.add_argument("--output", type=Path, help="Fichier de sortie")
    parser.add_argument("--output-dir", default="datasets", help="R√©pertoire de sortie")
    parser.add_argument("--no-cache", action="store_true", help="D√©sactiver d√©duplication")
    
    # Arguments ML
    parser.add_argument("--auto-label", action="store_true", help="Activer triple ML labeling PRODUIT")
    parser.add_argument("--ml-model", choices=["production", "sentiment", "importance", "fallback"], 
                       default="production", help="Mod√®le ML (compatibilit√©)")
    parser.add_argument("--confidence-threshold", type=float, default=0.75, 
                       help="Seuil de confiance ML")

    args = parser.parse_args()

    # V√©rifier la cl√© API
    if not os.getenv("FMP_API_KEY"):
        logger.error("‚ùå FMP_API_KEY environment variable required")
        return 1

    try:
        collector = SmartNewsCollector(
            output_dir=args.output_dir,
            enable_cache=not args.no_cache,
            auto_label=args.auto_label,
            ml_model=args.ml_model,
            confidence_threshold=args.confidence_threshold
        )

        output_file = collector.collect_and_save(
            count=args.count,
            days=args.days,
            output_file=args.output
        )

        print(f"‚úÖ Dataset Smart PRODUIT g√©n√©r√©: {output_file}")
        print(f"üéØ Colonnes: text, label (sentiment), importance, correlations")
        
        if args.auto_label:
            print(f"ü§ñ Triple ML PRODUIT:")
            print(f"  üòä Sentiment: {ML_MODELS_CONFIG['sentiment']}")
            print(f"  üéØ Importance: {ML_MODELS_CONFIG['importance']}")
            print(f"  üîó Corr√©lations: {ML_MODELS_CONFIG['correlation']} (ML) ou r√®gles")
        
        print("\nüöÄ Prochaines √©tapes:")
        print(f"  1. √âditer: open news_editor.html")
        print(f"  2. Commit: r√©entra√Æne les mod√®les avec nouvelles donn√©es")

    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
